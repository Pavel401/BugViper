{"type":"file-history-snapshot","messageId":"4c5bcbf3-0c3c-45bc-acca-967da32f73bf","snapshot":{"messageId":"4c5bcbf3-0c3c-45bc-acca-967da32f73bf","trackedFileBackups":{},"timestamp":"2026-02-13T07:21:39.287Z"},"isSnapshotUpdate":false}
{"parentUuid":null,"isSidechain":false,"userType":"external","cwd":"/Users/skmabudalam/Documents/BugViper","sessionId":"0f25fa6f-5425-4cfc-8c2e-2b3c7eda02c7","version":"2.1.39","gitBranch":"review-pipeline-improvements","type":"progress","data":{"type":"hook_progress","hookEvent":"SessionStart","hookName":"SessionStart:startup","command":"entire hooks claude-code session-start"},"parentToolUseID":"c610d3de-5aa8-4100-a9dd-6dfe5f605341","toolUseID":"c610d3de-5aa8-4100-a9dd-6dfe5f605341","timestamp":"2026-02-13T07:21:34.559Z","uuid":"d24cc371-2d11-4ffa-87c0-f6ccf4cab615"}
{"parentUuid":"d24cc371-2d11-4ffa-87c0-f6ccf4cab615","isSidechain":false,"userType":"external","cwd":"/Users/skmabudalam/Documents/BugViper","sessionId":"0f25fa6f-5425-4cfc-8c2e-2b3c7eda02c7","version":"2.1.39","gitBranch":"review-pipeline-improvements","type":"user","message":{"role":"user","content":"Hi"},"uuid":"4c5bcbf3-0c3c-45bc-acca-967da32f73bf","timestamp":"2026-02-13T07:21:36.220Z","thinkingMetadata":{"maxThinkingTokens":31999},"todos":[],"permissionMode":"default"}
{"parentUuid":"4c5bcbf3-0c3c-45bc-acca-967da32f73bf","isSidechain":false,"userType":"external","cwd":"/Users/skmabudalam/Documents/BugViper","sessionId":"0f25fa6f-5425-4cfc-8c2e-2b3c7eda02c7","version":"2.1.39","gitBranch":"review-pipeline-improvements","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_018mm7GFXoviNHXCQoVAgTey","type":"message","role":"assistant","content":[{"type":"text","text":"Hi! How can I help you with BugViper today?"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":4094,"cache_read_input_tokens":17854,"output_tokens":17,"server_tool_use":{"web_search_requests":0,"web_fetch_requests":0},"service_tier":"standard","cache_creation":{"ephemeral_1h_input_tokens":4094,"ephemeral_5m_input_tokens":0},"inference_geo":"","iterations":[]}},"requestId":"req_011CY5fWvcS8x97k4Y288Wpi","type":"assistant","uuid":"a82928bd-c1df-483c-88b9-cccf1a2bb5f4","timestamp":"2026-02-13T07:21:41.065Z"}
{"parentUuid":"a82928bd-c1df-483c-88b9-cccf1a2bb5f4","isSidechain":false,"userType":"external","cwd":"/Users/skmabudalam/Documents/BugViper","sessionId":"0f25fa6f-5425-4cfc-8c2e-2b3c7eda02c7","version":"2.1.39","gitBranch":"review-pipeline-improvements","type":"progress","data":{"type":"hook_progress","hookEvent":"Stop","hookName":"Stop","command":"entire hooks claude-code stop"},"parentToolUseID":"743cbd8e-a625-4774-b2a7-8f5139d391b3","toolUseID":"743cbd8e-a625-4774-b2a7-8f5139d391b3","timestamp":"2026-02-13T07:21:41.066Z","uuid":"a23e92f1-4f4b-4dea-b285-40a304ba58f9"}
{"parentUuid":"a23e92f1-4f4b-4dea-b285-40a304ba58f9","isSidechain":false,"userType":"external","cwd":"/Users/skmabudalam/Documents/BugViper","sessionId":"0f25fa6f-5425-4cfc-8c2e-2b3c7eda02c7","version":"2.1.39","gitBranch":"review-pipeline-improvements","type":"system","subtype":"stop_hook_summary","hookCount":1,"hookInfos":[{"command":"entire hooks claude-code stop"}],"hookErrors":[],"preventedContinuation":false,"stopReason":"","hasOutput":true,"level":"suggestion","timestamp":"2026-02-13T07:21:42.314Z","uuid":"7d2fa4d4-9bc3-4462-89ee-850b584b68e9","toolUseID":"743cbd8e-a625-4774-b2a7-8f5139d391b3"}
{"type":"file-history-snapshot","messageId":"849b236d-6663-4b93-be47-a871b6230169","snapshot":{"messageId":"849b236d-6663-4b93-be47-a871b6230169","trackedFileBackups":{},"timestamp":"2026-02-13T07:27:05.054Z"},"isSnapshotUpdate":false}
{"parentUuid":"7d2fa4d4-9bc3-4462-89ee-850b584b68e9","isSidechain":false,"userType":"external","cwd":"/Users/skmabudalam/Documents/BugViper","sessionId":"0f25fa6f-5425-4cfc-8c2e-2b3c7eda02c7","version":"2.1.39","gitBranch":"review-pipeline-improvements","type":"user","message":{"role":"user","content":"for each step store the results in a .md file , every time create a new folder called review-current time / inside this for each step in the execute_pr_review store the results in .md for now don't call the LLm , I only want to debug the context "},"uuid":"849b236d-6663-4b93-be47-a871b6230169","timestamp":"2026-02-13T07:27:02.521Z","thinkingMetadata":{"maxThinkingTokens":31999},"todos":[],"permissionMode":"default"}
{"parentUuid":"849b236d-6663-4b93-be47-a871b6230169","isSidechain":false,"userType":"external","cwd":"/Users/skmabudalam/Documents/BugViper","sessionId":"0f25fa6f-5425-4cfc-8c2e-2b3c7eda02c7","version":"2.1.39","gitBranch":"review-pipeline-improvements","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_011UFUca87YBzuhqvycUn4YR","type":"message","role":"assistant","content":[{"type":"thinking","thinking":"The user wants to modify the `execute_pr_review` function to:\n1. Create a new folder called `review-{current_time}/` \n2. For each step in the function, store the results in `.md` files inside that folder\n3. Don't call the LLM - just debug the context\n\nLet me first read the relevant files to understand the current implementation.","signature":"EvIDCkYICxgCKkA8wpx8oMLrlGyDT73qFiFEbPdW7d8FsIQszMRh+3W0jsrTIBVe3CO5fF94/RNyIB95OxxSbg8yEqfjn64loTuWEgxIkQe/pxn5oIatiFEaDImG+F984d6TOjRQ/CIwXE1gxtljvT1lCnXM+w1tFUSJ7sKuVZ/9SkKk8L4Nxbm1Az7abNU+xiHrKzRZg9VGKtkCkj5oUbcg9eq54rBntEIRx3O8Q4G5+g0x3K0MbNcWxiFKaQXyXijgwshDpfSZ4//sCPVDIMDAA3Lc64kAg/1YqiXdKgYS6RhyrZzFtG9j3tW/jAliTHj8ZUnOTXq5ry8I57RdaffSMFejDSavqOiFhlurhKgqN73X1M2gVZKbMI1lrJv6j7SeYQXUIdINfFErGupimF+4j9MkATE1zcEiG2HtwSmVIqbAt7wVgkUfSk95QSLiYHr4qNAdQwOB34mOBfhkuAuj1QGgPHFS3JMq5P5QfRzZhiXHQkEbtQAWpQcFUeJkBTy4nfQE+U4WY45LOfnm9AMSbIkNRSRA0SNGh7S2SEtwRvdrNlr5gyr7Fw+sELP+92OEECEO9V2P+SIzURYnMEDUQ17XgqgVesf5weqBTCKtjjwEdMCrtW3uVL5zudzxy35DXoV1qmv/Bt94P1cx7TbakOnnGAE="}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":80,"cache_read_input_tokens":21948,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":80},"output_tokens":0,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY5fvwdsNmDiBBBPakTGn","type":"assistant","uuid":"c6aa6149-7675-4688-8342-ea10066f14e4","timestamp":"2026-02-13T07:27:07.692Z"}
{"parentUuid":"c6aa6149-7675-4688-8342-ea10066f14e4","isSidechain":false,"userType":"external","cwd":"/Users/skmabudalam/Documents/BugViper","sessionId":"0f25fa6f-5425-4cfc-8c2e-2b3c7eda02c7","version":"2.1.39","gitBranch":"review-pipeline-improvements","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_011UFUca87YBzuhqvycUn4YR","type":"message","role":"assistant","content":[{"type":"text","text":"Let me read the relevant files first."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":80,"cache_read_input_tokens":21948,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":80},"output_tokens":0,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY5fvwdsNmDiBBBPakTGn","type":"assistant","uuid":"ddd70eab-9162-4fbe-9302-1964d40149ce","timestamp":"2026-02-13T07:27:07.909Z"}
{"parentUuid":"ddd70eab-9162-4fbe-9302-1964d40149ce","isSidechain":false,"userType":"external","cwd":"/Users/skmabudalam/Documents/BugViper","sessionId":"0f25fa6f-5425-4cfc-8c2e-2b3c7eda02c7","version":"2.1.39","gitBranch":"review-pipeline-improvements","slug":"immutable-launching-music","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_011UFUca87YBzuhqvycUn4YR","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01QxPJ9unb5oyyQUcc123C2n","name":"Read","input":{"file_path":"/Users/skmabudalam/Documents/BugViper/api/services/review_service.py"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":80,"cache_read_input_tokens":21948,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":80},"output_tokens":0,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY5fvwdsNmDiBBBPakTGn","type":"assistant","uuid":"bad2c35d-5174-4251-90cd-448db8a63749","timestamp":"2026-02-13T07:27:08.429Z"}
{"parentUuid":"bad2c35d-5174-4251-90cd-448db8a63749","isSidechain":false,"userType":"external","cwd":"/Users/skmabudalam/Documents/BugViper","sessionId":"0f25fa6f-5425-4cfc-8c2e-2b3c7eda02c7","version":"2.1.39","gitBranch":"review-pipeline-improvements","slug":"immutable-launching-music","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Read","command":"callback"},"parentToolUseID":"toolu_01QxPJ9unb5oyyQUcc123C2n","toolUseID":"toolu_01QxPJ9unb5oyyQUcc123C2n","timestamp":"2026-02-13T07:27:08.435Z","uuid":"35589d31-3e25-474c-9348-132711018b4f"}
{"parentUuid":"bad2c35d-5174-4251-90cd-448db8a63749","isSidechain":false,"userType":"external","cwd":"/Users/skmabudalam/Documents/BugViper","sessionId":"0f25fa6f-5425-4cfc-8c2e-2b3c7eda02c7","version":"2.1.39","gitBranch":"review-pipeline-improvements","slug":"immutable-launching-music","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01QxPJ9unb5oyyQUcc123C2n","type":"tool_result","content":"     1→\"\"\"\n     2→PR Review Service\n     3→=================\n     4→Orchestrates the full review pipeline:\n     5→  1. Fetch PR diff from GitHub\n     6→  2. Parse diff → extract added source per file\n     7→  3. Use ingestion_service language parsers (tree-sitter) to extract imports & symbols\n     8→  4. Resolve those imports against the Neo4j graph (same repo)\n     9→  5. Find callers of changed functions (impact analysis)\n    10→  6. Build context → run LLM agents → post comment\n    11→\"\"\"\n    12→\n    13→import importlib\n    14→import logging\n    15→import os\n    16→import traceback\n    17→from datetime import datetime\n    18→from pathlib import Path\n    19→from typing import Dict, List, Set, Tuple\n    20→\n    21→from api.utils.comment_formatter import format_github_comment\n    22→from api.utils.graph_context import build_graph_context_section\n    23→from db.client import Neo4jClient\n    24→from db.queries import CodeQueryService\n    25→from deepagent.models.agent_schemas import ContextData\n    26→from deepagent.agent.review_pipeline import run_review\n    27→from common.github_client import GitHubClient\n    28→from common.diff_parser import parse_unified_diff\n    29→\n    30→logger = logging.getLogger(__name__)\n    31→\n    32→\n    33→# ==========================================================================\n    34→# File extension → tree-sitter language name mapping (all 17 supported)\n    35→# ==========================================================================\n    36→_EXT_TO_LANG = {\n    37→    \".py\": \"python\",\n    38→    \".js\": \"javascript\", \".jsx\": \"javascript\", \".mjs\": \"javascript\", \".cjs\": \"javascript\",\n    39→    \".ts\": \"typescript\", \".tsx\": \"typescript\",\n    40→    \".go\": \"go\",\n    41→    \".rs\": \"rust\",\n    42→    \".java\": \"java\",\n    43→    \".rb\": \"ruby\",\n    44→    \".c\": \"c\",\n    45→    \".cpp\": \"cpp\", \".h\": \"cpp\", \".hpp\": \"cpp\",\n    46→    \".cs\": \"c_sharp\",\n    47→    \".kt\": \"kotlin\",\n    48→    \".scala\": \"scala\", \".sc\": \"scala\",\n    49→    \".swift\": \"swift\",\n    50→    \".php\": \"php\",\n    51→    \".hs\": \"haskell\",\n    52→}\n    53→\n    54→# ==========================================================================\n    55→# Language parser registry\n    56→# Maps language name → (module path, class name) for lazy import.\n    57→# These are the SAME parsers used by the ingestion service to build the graph.\n    58→# ==========================================================================\n    59→_LANG_PARSER_REGISTRY = {\n    60→    \"python\":     (\"ingestion_service.languages.python\",     \"PythonLangTreeSitterParser\"),\n    61→    \"javascript\": (\"ingestion_service.languages.javascript\", \"JavascriptLangTreeSitterParser\"),\n    62→    \"typescript\": (\"ingestion_service.languages.typescript\", \"TypescriptLangTreeSitterParser\"),\n    63→    \"go\":         (\"ingestion_service.languages.go\",         \"GoLangTreeSitterParser\"),\n    64→    \"java\":       (\"ingestion_service.languages.java\",       \"JavaLangTreeSitterParser\"),\n    65→    \"rust\":       (\"ingestion_service.languages.rust\",       \"RustLangTreeSitterParser\"),\n    66→    \"c\":          (\"ingestion_service.languages.c\",          \"CLangTreeSitterParser\"),\n    67→    \"cpp\":        (\"ingestion_service.languages.cpp\",        \"CppLangTreeSitterParser\"),\n    68→    \"ruby\":       (\"ingestion_service.languages.ruby\",       \"RubyLangTreeSitterParser\"),\n    69→    \"c_sharp\":    (\"ingestion_service.languages.csharp\",     \"CSharpLangTreeSitterParser\"),\n    70→    \"php\":        (\"ingestion_service.languages.php\",        \"PhpLangTreeSitterParser\"),\n    71→    \"kotlin\":     (\"ingestion_service.languages.kotlin\",     \"KotlinLangTreeSitterParser\"),\n    72→    \"scala\":      (\"ingestion_service.languages.scala\",      \"ScalaLangTreeSitterParser\"),\n    73→    \"swift\":      (\"ingestion_service.languages.swift\",      \"SwiftLangTreeSitterParser\"),\n    74→    \"haskell\":    (\"ingestion_service.languages.haskell\",    \"HaskellLangTreeSitterParser\"),\n    75→}\n    76→\n    77→# Cache so we only instantiate each language parser once\n    78→_parser_cache: Dict[str, object] = {}\n    79→\n    80→\n    81→# ==========================================================================\n    82→# Parser loading\n    83→# ==========================================================================\n    84→\n    85→def _get_lang_parser(lang: str):\n    86→    \"\"\"\n    87→    Get the tree-sitter language parser for `lang`.\n    88→\n    89→    Each language parser (e.g. PythonLangTreeSitterParser) expects a wrapper\n    90→    with .language_name, .language, and .parser attributes.  We build a\n    91→    lightweight adapter instead of pulling in the full ingestion TreeSitterParser\n    92→    which depends on Neo4j/jobs.\n    93→    \"\"\"\n    94→    # Return from cache if already loaded\n    95→    if lang in _parser_cache:\n    96→        return _parser_cache[lang]\n    97→\n    98→    # Unknown language → skip\n    99→    if lang not in _LANG_PARSER_REGISTRY:\n   100→        return None\n   101→\n   102→    module_path, class_name = _LANG_PARSER_REGISTRY[lang]\n   103→    try:\n   104→        from common.tree_sitter_manager import get_language_safe, create_parser\n   105→\n   106→        # Build the adapter the language parser constructors expect\n   107→        class _Adapter:\n   108→            pass\n   109→        adapter = _Adapter()\n   110→        adapter.language_name = lang\n   111→        adapter.language = get_language_safe(lang)   # tree-sitter Language object\n   112→        adapter.parser = create_parser(lang)         # tree-sitter Parser object\n   113→\n   114→        # Dynamically import the language-specific parser class\n   115→        module = importlib.import_module(module_path)\n   116→        parser_class = getattr(module, class_name)\n   117→\n   118→        # Instantiate and configure\n   119→        parser_instance = parser_class(adapter)\n   120→        parser_instance.index_source = False  # we don't need source code stored on each node\n   121→\n   122→        _parser_cache[lang] = parser_instance\n   123→        return parser_instance\n   124→    except Exception as e:\n   125→        logger.warning(f\"Failed to load parser for {lang}: {e}\")\n   126→        return None\n   127→\n   128→\n   129→# ==========================================================================\n   130→# Diff parsing helpers\n   131→# ==========================================================================\n   132→\n   133→def _extract_added_source_by_file(diff_text: str) -> Dict[str, str]:\n   134→    \"\"\"\n   135→    Walk through a unified diff and collect added lines per file.\n   136→\n   137→    Returns {\"app/services/foo.py\": \"from x import y\\\\ndef bar(): ...\"}\n   138→\n   139→    Only lines starting with '+' (but not '+++ b/...') are collected,\n   140→    with the leading '+' stripped so tree-sitter can parse the source.\n   141→    \"\"\"\n   142→    files: Dict[str, List[str]] = {}\n   143→    current_file = None\n   144→\n   145→    for line in diff_text.splitlines():\n   146→        # \"+++ b/path/to/file\" marks the start of a new file in the diff\n   147→        if line.startswith(\"+++ b/\"):\n   148→            current_file = line[6:]  # strip \"+++ b/\" prefix\n   149→            if current_file not in files:\n   150→                files[current_file] = []\n   151→        # Lines starting with \"+\" are added lines (skip the \"+++ b/\" header)\n   152→        elif line.startswith(\"+\") and not line.startswith(\"+++\") and current_file:\n   153→            files[current_file].append(line[1:])  # strip the leading \"+\"\n   154→\n   155→    # Join lines into a single source string per file, skip empty files\n   156→    return {f: \"\\n\".join(lines) for f, lines in files.items() if lines}\n   157→\n   158→\n   159→# ==========================================================================\n   160→# Import & symbol extraction from diff (tree-sitter powered)\n   161→# ==========================================================================\n   162→\n   163→def extract_imports_from_diff(diff_text: str) -> List[Dict]:\n   164→    \"\"\"\n   165→    Extract imports from the added lines of a diff using tree-sitter.\n   166→\n   167→    Uses the same language parsers as the ingestion service, so the import\n   168→    format matches what the graph stores:\n   169→      Python:     {\"name\": \"foo\", \"full_import_name\": \"bar.foo\", ...}\n   170→      JavaScript: {\"name\": \"useState\", \"source\": \"react\", ...}\n   171→    \"\"\"\n   172→    all_imports: List[Dict] = []\n   173→\n   174→    # Group added source by file path\n   175→    added_source = _extract_added_source_by_file(diff_text)\n   176→\n   177→    for file_path, source in added_source.items():\n   178→        # Determine language from file extension\n   179→        ext = Path(file_path).suffix.lower()\n   180→        lang = _EXT_TO_LANG.get(ext)\n   181→        if not lang:\n   182→            continue\n   183→\n   184→        # Load the language-specific parser\n   185→        parser = _get_lang_parser(lang)\n   186→        if not parser or not hasattr(parser, \"_find_imports\"):\n   187→            continue\n   188→\n   189→        try:\n   190→            # Parse the added source into a tree-sitter AST\n   191→            tree = parser.parser.parse(source.encode(\"utf-8\"))\n   192→            # Use the parser's own _find_imports to extract structured import data\n   193→            file_imports = parser._find_imports(tree.root_node)\n   194→            all_imports.extend(file_imports)\n   195→        except Exception as e:\n   196→            logger.warning(f\"Import extraction failed for {file_path}: {e}\")\n   197→\n   198→    return all_imports\n   199→\n   200→\n   201→def extract_symbols_from_diff(diff_text: str) -> Tuple[Set[str], Set[str]]:\n   202→    \"\"\"\n   203→    Extract function and class names defined in the added lines of a diff.\n   204→\n   205→    Returns (function_names, class_names) as sets of strings.\n   206→    \"\"\"\n   207→    functions: Set[str] = set()\n   208→    classes: Set[str] = set()\n   209→\n   210→    added_source = _extract_added_source_by_file(diff_text)\n   211→\n   212→    print('----- The Added source for the diff -----')\n   213→    print(added_source)\n   214→    print('-----------------------------------------------------')\n   215→\n   216→    for file_path, source in added_source.items():\n   217→        #get the file extension\n   218→        ext = Path(file_path).suffix.lower()\n   219→\n   220→        #get the language from the file extension\n   221→        lang = _EXT_TO_LANG.get(ext)\n   222→        if not lang:\n   223→            continue\n   224→\n   225→        #get the language-specific parser\n   226→        parser = _get_lang_parser(lang)\n   227→        if not parser:\n   228→            continue\n   229→\n   230→        try:\n   231→            #parse the added source into a tree-sitter AST\n   232→            tree = parser.parser.parse(source.encode(\"utf-8\"))\n   233→            root = tree.root_node\n   234→\n   235→            # Extract function definitions\n   236→            if hasattr(parser, \"_find_functions\"):\n   237→                for func in parser._find_functions(root):\n   238→                    functions.add(func.get(\"name\", \"\"))\n   239→\n   240→            # Extract class definitions\n   241→            if hasattr(parser, \"_find_classes\"):\n   242→                for cls in parser._find_classes(root):\n   243→                    classes.add(cls.get(\"name\", \"\"))\n   244→        except Exception as e:\n   245→            logger.warning(f\"Symbol extraction failed for {file_path}: {e}\")\n   246→\n   247→    return functions, classes\n   248→\n   249→\n   250→# ==========================================================================\n   251→# Graph-based resolution\n   252→# ==========================================================================\n   253→\n   254→def _resolve_imports_from_graph(\n   255→    neo4j_client: Neo4jClient, imports: List[Dict], repo_id: str,\n   256→) -> List[Dict]:\n   257→    \"\"\"\n   258→    For each imported name, search the Neo4j graph for its definition.\n   259→\n   260→    Queries directly with repo filtering (WHERE n.repo = $repo) so results\n   261→    are scoped to the PR's repository. Searches Function, Class, and Variable\n   262→    nodes by exact name match.\n   263→\n   264→    Returns a list of resolved import dicts with source code from the graph.\n   265→    \"\"\"\n   266→    resolved = []\n   267→    seen = set()  # avoid looking up the same name twice\n   268→\n   269→    with neo4j_client.driver.session() as session:\n   270→        for imp in imports:\n   271→            name = imp.get(\"name\", \"\")\n   272→\n   273→            # Skip empty, already-seen, or wildcard imports\n   274→            if not name or name in seen or name == \"*\":\n   275→                continue\n   276→            seen.add(name)\n   277→\n   278→            # full_import_name for Python (\"app.services.foo.bar\"),\n   279→            # source for JS/TS (\"react\", \"./utils\")\n   280→            module = imp.get(\"full_import_name\", imp.get(\"source\", \"\"))\n   281→\n   282→            try:\n   283→                # Search for the symbol in the graph, scoped to this repo\n   284→                result = session.run(\"\"\"\n   285→                    MATCH (n)\n   286→                    WHERE (n:Function OR n:Class OR n:Variable)\n   287→                      AND n.name = $name\n   288→                      AND n.repo = $repo\n   289→                    RETURN n.name AS name, n.path AS path,\n   290→                           n.line_number AS line_number,\n   291→                           n.source AS source, n.docstring AS docstring,\n   292→                           labels(n) AS labels\n   293→                    LIMIT 1\n   294→                \"\"\", name=name, repo=repo_id)\n   295→\n   296→                records = result.data()\n   297→                if not records:\n   298→                    continue  # symbol not in graph (external dep or not ingested)\n   299→\n   300→                r = records[0]\n   301→                labels = r.get(\"labels\", [])\n   302→                node_type = \"class\" if \"Class\" in labels else \"function\"\n   303→\n   304→                resolved.append({\n   305→                    \"name\": name,\n   306→                    \"module\": module,\n   307→                    \"type\": node_type,\n   308→                    \"source\": r.get(\"source\", \"\"),\n   309→                    \"path\": r.get(\"path\", \"\"),\n   310→                    \"line\": r.get(\"line_number\"),\n   311→                    \"docstring\": r.get(\"docstring\", \"\"),\n   312→                })\n   313→            except Exception as e:\n   314→                logger.warning(f\"Failed to resolve import {name}: {e}\")\n   315→\n   316→    return resolved\n   317→\n   318→\n   319→def _find_callers(code_finder, symbol_names: Set[str]) -> List[Dict]:\n   320→    \"\"\"\n   321→    For each changed symbol, find functions that call it (impact analysis).\n   322→\n   323→    Uses CodeFinder.who_calls_function which queries CALLS relationships\n   324→    in the graph.  Returns at most 10 callers per symbol.\n   325→    \"\"\"\n   326→    callers = []\n   327→\n   328→    for name in symbol_names:\n   329→        try:\n   330→            results = code_finder.who_calls_function(name)\n   331→            if not results:\n   332→                continue\n   333→\n   334→            # Extract caller info, limit to 10 per symbol\n   335→            caller_list = [\n   336→                {\n   337→                    \"name\": c.get(\"caller_function\"),\n   338→                    \"path\": c.get(\"caller_file_path\"),\n   339→                    \"line\": c.get(\"caller_line_number\"),\n   340→                }\n   341→                for c in results\n   342→                if c.get(\"caller_function\")\n   343→            ][:10]\n   344→\n   345→            if caller_list:\n   346→                callers.append({\"symbol\": name, \"callers\": caller_list})\n   347→        except Exception as e:\n   348→            logger.warning(f\"Failed to find callers for {name}: {e}\")\n   349→\n   350→    return callers\n   351→\n   352→\n   353→# ==========================================================================\n   354→# Context formatting (for LLM agents)\n   355→# ==========================================================================\n   356→\n   357→def _build_agent_context(\n   358→    import_context: List[Dict],\n   359→    caller_context: List[Dict],\n   360→    graph_section: str,\n   361→) -> str:\n   362→    \"\"\"\n   363→    Build the markdown context string that gets sent to the LLM review agents.\n   364→\n   365→    Combines:\n   366→      - Resolved import source code from the graph\n   367→      - Impact analysis (who calls the changed code)\n   368→      - Graph context (symbols in changed line ranges)\n   369→    \"\"\"\n   370→    parts = []\n   371→\n   372→    # Section 1: Source code of imported dependencies\n   373→    if import_context:\n   374→        parts.append(\"## Imported Dependencies (source from graph)\")\n   375→        for imp in import_context:\n   376→            source = imp.get(\"source\", \"\")\n   377→            if source:\n   378→                parts.append(f\"### `{imp['name']}` from `{imp['module']}` ({imp.get('path', '')})\")\n   379→                parts.append(f\"```python\\n{source}\\n```\")\n   380→            else:\n   381→                parts.append(f\"- `{imp['name']}` from `{imp['module']}` (source not in graph)\")\n   382→        parts.append(\"\")\n   383→\n   384→    # Section 2: Callers of changed functions\n   385→    if caller_context:\n   386→        parts.append(\"## Impact Analysis (who calls the changed code)\")\n   387→        for entry in caller_context:\n   388→            caller_names = \", \".join(\n   389→                f\"`{c['name']}` in `{c.get('path', '?')}`\" for c in entry[\"callers\"]\n   390→            )\n   391→            parts.append(f\"- `{entry['symbol']}` is called by: {caller_names}\")\n   392→        parts.append(\"\")\n   393→\n   394→    # Section 3: Graph context (symbols overlapping changed lines)\n   395→    if graph_section and graph_section != \"No graph context available.\":\n   396→        parts.append(\"## Graph Context\")\n   397→        parts.append(graph_section)\n   398→        parts.append(\"\")\n   399→\n   400→    return \"\\n\".join(parts) if parts else \"No additional context available.\"\n   401→\n   402→\n   403→# ==========================================================================\n   404→# Debug dump (writes context to output/<pr_number>/context_<timestamp>.md)\n   405→# ==========================================================================\n   406→\n   407→def _dump_context_to_file(\n   408→    owner: str, repo: str, pr_number: int, diff_text: str,\n   409→    agent_context: str, raw_imports: List[Dict], resolved_imports: List[Dict],\n   410→    caller_context: List[Dict], graph_symbols: list,\n   411→    files_changed: list, changed_symbol_names: list, risk_level: str,\n   412→) -> None:\n   413→    \"\"\"Save all gathered context to a markdown file for debugging/inspection.\"\"\"\n   414→    try:\n   415→        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n   416→        output_dir = Path(\"output\") / str(pr_number)\n   417→        output_dir.mkdir(parents=True, exist_ok=True)\n   418→        context_file = output_dir / f\"context_{timestamp}.md\"\n   419→\n   420→        total_callers = sum(len(c[\"callers\"]) for c in caller_context)\n   421→\n   422→        lines = [\n   423→            f\"# PR Review Context — {owner}/{repo}#{pr_number}\",\n   424→            f\"**Generated at:** {datetime.now().isoformat()}\",\n   425→            f\"**Risk level:** {risk_level}\",\n   426→            \"\",\n   427→            # What files were touched\n   428→            \"## Files Changed\",\n   429→            *[f\"- `{f}`\" for f in files_changed],\n   430→            \"\",\n   431→            # Functions/classes defined in the added lines\n   432→            \"## Changed Symbols (from diff)\",\n   433→            *[f\"- `{s}`\" for s in changed_symbol_names],\n   434→            \"\",\n   435→            # All imports found in the added lines (tree-sitter extraction)\n   436→            f\"## Extracted Imports from Diff ({len(raw_imports)})\",\n   437→            *[f\"- `{imp.get('name', '?')}` (full: `{imp.get('full_import_name', imp.get('source', '?'))}`)\"\n   438→              for imp in raw_imports],\n   439→            \"\",\n   440→            # Which of those imports we found in the graph (with source code)\n   441→            f\"## Resolved Imports ({len(resolved_imports)}/{len(raw_imports)} resolved from graph)\",\n   442→            *[f\"- `{imp['name']}` from `{imp['module']}` → `{imp.get('path', 'not found')}`\"\n   443→              for imp in resolved_imports],\n   444→            \"\",\n   445→            # Functions that call the changed symbols\n   446→            f\"## Caller Impact ({total_callers} callers)\",\n   447→            *[f\"- `{c['symbol']}` called by: \"\n   448→              + \", \".join(f\"`{caller['name']}`\" for caller in c[\"callers\"])\n   449→              for c in caller_context],\n   450→            \"\",\n   451→            # Symbols from the graph that overlap with changed line ranges\n   452→            f\"## Graph Symbols ({len(graph_symbols)})\",\n   453→            *[f\"- `{s.get('name', '')}` ({s.get('type', 'unknown')}) in `{s.get('file_path', '')}`\"\n   454→              for s in graph_symbols],\n   455→            \"\",\n   456→            # The full context string that gets sent to the LLM agents\n   457→            \"## Context Sent to Agents\",\n   458→            agent_context,\n   459→            \"\",\n   460→            # Raw diff for reference\n   461→            \"## Diff\",\n   462→            \"```diff\",\n   463→            diff_text,\n   464→            \"```\",\n   465→        ]\n   466→\n   467→        context_file.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n   468→        logger.info(f\"Review context saved to {context_file}\")\n   469→    except Exception as e:\n   470→        logger.warning(f\"Failed to dump review context: {e}\")\n   471→\n   472→\n   473→# ==========================================================================\n   474→# Main pipeline\n   475→# ==========================================================================\n   476→\n   477→async def execute_pr_review(owner: str, repo: str, pr_number: int) -> None:\n   478→    \"\"\"\n   479→    Full PR review pipeline.\n   480→\n   481→    Steps:\n   482→      1. Fetch the PR diff from GitHub\n   483→      2. Parse the diff into changed file paths and line ranges\n   484→      3. Extract imports and symbol definitions from added lines (tree-sitter)\n   485→      4. Connect to Neo4j and resolve context:\n   486→         a. Find graph symbols overlapping changed line ranges\n   487→         b. Look up imported symbols in the graph (get source code)\n   488→         c. Find callers of changed functions (impact analysis)\n   489→      5. Build risk level based on scope of changes\n   490→      6. Format everything into context for the LLM agents\n   491→      7. Dump context to file for debugging\n   492→      8. Run multi-agent review (bug-hunter + security-auditor)\n   493→      9. Format and post the review comment on the PR\n   494→    \"\"\"\n   495→    try:\n   496→        logger.info(f\"Starting review pipeline for {owner}/{repo}#{pr_number}\")\n   497→        repo_id = f\"{owner}/{repo}\"  # matches the repo identifier stored in the graph\n   498→\n   499→        # ── Step 1: Fetch diff from GitHub ──────────────────────────────\n   500→        gh = GitHubClient()\n   501→        diff_text = await gh.get_pr_diff(owner, repo, pr_number)\n   502→        if not diff_text:\n   503→            logger.warning(\"Empty diff, skipping review\")\n   504→            return\n   505→        logger.info(f\"Fetched diff ({len(diff_text)} chars)\")\n   506→\n   507→        # ── Step 2: Parse diff structure ────────────────────────────────\n   508→        # Returns list of {\"file_path\", \"start_line\", \"end_line\"} hunks\n   509→        changes = parse_unified_diff(diff_text)\n   510→        files_changed = list({c[\"file_path\"] for c in changes})\n   511→        logger.info(f\"Parsed {len(changes)} hunks across {len(files_changed)} files\")\n   512→\n   513→        # ── Step 3: Extract imports & symbols from added lines ──────────\n   514→        # Uses the same tree-sitter language parsers as the ingestion service\n   515→        diff_imports = extract_imports_from_diff(diff_text)\n   516→\n   517→        print('-----------------------------------------------------')\n   518→        print(diff_imports)\n   519→        print('-----------------------------------------------------')\n   520→\n   521→        diff_functions, diff_classes = extract_symbols_from_diff(diff_text)\n   522→\n   523→        print('-----------------------------------------------------')\n   524→        print(diff_functions)\n   525→        print(diff_classes)\n   526→        print('-----------------------------------------------------')\n   527→\n   528→    \n   529→        all_diff_symbols = diff_functions | diff_classes\n   530→        logger.info(\n   531→            f\"Diff extraction: {len(diff_imports)} imports, \"\n   532→            f\"{len(diff_functions)} functions, {len(diff_classes)} classes\"\n   533→        )\n   534→\n   535→        # ── Step 4: Connect to Neo4j and resolve context ────────────────\n   536→        neo4j = Neo4jClient(\n   537→            uri=os.environ.get(\"NEO4J_URI\", \"\"),\n   538→            user=os.environ.get(\"NEO4J_USERNAME\", \"neo4j\"),\n   539→            password=os.environ.get(\"NEO4J_PASSWORD\", \"\"),\n   540→            database=os.environ.get(\"NEO4J_DATABASE\", \"neo4j\"),\n   541→        )\n   542→        query_service = CodeQueryService(neo4j)\n   543→        from api.services.code_search import CodeFinder\n   544→        code_finder = CodeFinder(neo4j)\n   545→\n   546→        # 4a. Graph symbols overlapping the changed line ranges\n   547→        graph_context = query_service.get_diff_context_enhanced(repo_id, changes)\n   548→        graph_symbols = graph_context.get(\"affected_symbols\", [])\n   549→        graph_section = build_graph_context_section(graph_context)\n   550→\n   551→        # 4b. Look up each imported name in the graph (scoped to this repo)\n   552→        resolved_imports = _resolve_imports_from_graph(neo4j, diff_imports, repo_id)\n   553→        logger.info(f\"Resolved {len(resolved_imports)}/{len(diff_imports)} imports from graph\")\n   554→\n   555→        # 4c. Find callers of changed functions (impact analysis)\n   556→        # Merge symbols from diff extraction + graph overlap\n   557→        all_symbol_names = all_diff_symbols | {s.get(\"name\", \"\") for s in graph_symbols}\n   558→        caller_context = _find_callers(code_finder, all_symbol_names)\n   559→        total_callers = sum(len(c[\"callers\"]) for c in caller_context)\n   560→        logger.info(f\"Found {total_callers} callers across {len(caller_context)} symbols\")\n   561→\n   562→        # ── Step 5: Determine risk level ────────────────────────────────\n   563→        changed_symbol_names = list(all_symbol_names)\n   564→        if total_callers > 10 or len(all_symbol_names) > 5:\n   565→            risk_level = \"high\"\n   566→        elif total_callers > 3 or len(all_symbol_names) > 2:\n   567→            risk_level = \"medium\"\n   568→        else:\n   569→            risk_level = \"low\"\n   570→\n   571→        # ── Step 6: Build context for LLM agents ───────────────────────\n   572→        agent_context = _build_agent_context(\n   573→            resolved_imports, caller_context, graph_section\n   574→        )\n   575→\n   576→        # ── Step 7: Dump context to file for debugging ──────────────────\n   577→        _dump_context_to_file(\n   578→            owner, repo, pr_number, diff_text,\n   579→            agent_context, diff_imports, resolved_imports, caller_context,\n   580→            graph_symbols, files_changed, changed_symbol_names, risk_level,\n   581→        )\n   582→\n   583→        # ── Step 8: Run multi-agent review ──────────────────────────────\n   584→        review_results = await run_review(diff_text, agent_context, repo_id, pr_number)\n   585→        logger.info(f\"Review complete: {len(review_results.issues)} issues found\")\n   586→\n   587→        # ── Step 9: Format and post comment ─────────────────────────────\n   588→        context_data = ContextData(\n   589→            files_changed=files_changed,\n   590→            modified_symbols=changed_symbol_names,\n   591→            total_callers=total_callers,\n   592→            risk_level=risk_level,\n   593→        )\n   594→        final_comment = format_github_comment(review_results, context_data, pr_number)\n   595→        await gh.post_comment(owner, repo, pr_number, final_comment)\n   596→        logger.info(f\"Posted review comment on {owner}/{repo}#{pr_number}\")\n   597→\n   598→    except Exception:\n   599→        logger.error(f\"Review pipeline failed: {traceback.format_exc()}\")\n   600→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"caf6a7cc-7daa-4b40-a48a-48c697325d74","timestamp":"2026-02-13T07:27:08.435Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/skmabudalam/Documents/BugViper/api/services/review_service.py","content":"\"\"\"\nPR Review Service\n=================\nOrchestrates the full review pipeline:\n  1. Fetch PR diff from GitHub\n  2. Parse diff → extract added source per file\n  3. Use ingestion_service language parsers (tree-sitter) to extract imports & symbols\n  4. Resolve those imports against the Neo4j graph (same repo)\n  5. Find callers of changed functions (impact analysis)\n  6. Build context → run LLM agents → post comment\n\"\"\"\n\nimport importlib\nimport logging\nimport os\nimport traceback\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, List, Set, Tuple\n\nfrom api.utils.comment_formatter import format_github_comment\nfrom api.utils.graph_context import build_graph_context_section\nfrom db.client import Neo4jClient\nfrom db.queries import CodeQueryService\nfrom deepagent.models.agent_schemas import ContextData\nfrom deepagent.agent.review_pipeline import run_review\nfrom common.github_client import GitHubClient\nfrom common.diff_parser import parse_unified_diff\n\nlogger = logging.getLogger(__name__)\n\n\n# ==========================================================================\n# File extension → tree-sitter language name mapping (all 17 supported)\n# ==========================================================================\n_EXT_TO_LANG = {\n    \".py\": \"python\",\n    \".js\": \"javascript\", \".jsx\": \"javascript\", \".mjs\": \"javascript\", \".cjs\": \"javascript\",\n    \".ts\": \"typescript\", \".tsx\": \"typescript\",\n    \".go\": \"go\",\n    \".rs\": \"rust\",\n    \".java\": \"java\",\n    \".rb\": \"ruby\",\n    \".c\": \"c\",\n    \".cpp\": \"cpp\", \".h\": \"cpp\", \".hpp\": \"cpp\",\n    \".cs\": \"c_sharp\",\n    \".kt\": \"kotlin\",\n    \".scala\": \"scala\", \".sc\": \"scala\",\n    \".swift\": \"swift\",\n    \".php\": \"php\",\n    \".hs\": \"haskell\",\n}\n\n# ==========================================================================\n# Language parser registry\n# Maps language name → (module path, class name) for lazy import.\n# These are the SAME parsers used by the ingestion service to build the graph.\n# ==========================================================================\n_LANG_PARSER_REGISTRY = {\n    \"python\":     (\"ingestion_service.languages.python\",     \"PythonLangTreeSitterParser\"),\n    \"javascript\": (\"ingestion_service.languages.javascript\", \"JavascriptLangTreeSitterParser\"),\n    \"typescript\": (\"ingestion_service.languages.typescript\", \"TypescriptLangTreeSitterParser\"),\n    \"go\":         (\"ingestion_service.languages.go\",         \"GoLangTreeSitterParser\"),\n    \"java\":       (\"ingestion_service.languages.java\",       \"JavaLangTreeSitterParser\"),\n    \"rust\":       (\"ingestion_service.languages.rust\",       \"RustLangTreeSitterParser\"),\n    \"c\":          (\"ingestion_service.languages.c\",          \"CLangTreeSitterParser\"),\n    \"cpp\":        (\"ingestion_service.languages.cpp\",        \"CppLangTreeSitterParser\"),\n    \"ruby\":       (\"ingestion_service.languages.ruby\",       \"RubyLangTreeSitterParser\"),\n    \"c_sharp\":    (\"ingestion_service.languages.csharp\",     \"CSharpLangTreeSitterParser\"),\n    \"php\":        (\"ingestion_service.languages.php\",        \"PhpLangTreeSitterParser\"),\n    \"kotlin\":     (\"ingestion_service.languages.kotlin\",     \"KotlinLangTreeSitterParser\"),\n    \"scala\":      (\"ingestion_service.languages.scala\",      \"ScalaLangTreeSitterParser\"),\n    \"swift\":      (\"ingestion_service.languages.swift\",      \"SwiftLangTreeSitterParser\"),\n    \"haskell\":    (\"ingestion_service.languages.haskell\",    \"HaskellLangTreeSitterParser\"),\n}\n\n# Cache so we only instantiate each language parser once\n_parser_cache: Dict[str, object] = {}\n\n\n# ==========================================================================\n# Parser loading\n# ==========================================================================\n\ndef _get_lang_parser(lang: str):\n    \"\"\"\n    Get the tree-sitter language parser for `lang`.\n\n    Each language parser (e.g. PythonLangTreeSitterParser) expects a wrapper\n    with .language_name, .language, and .parser attributes.  We build a\n    lightweight adapter instead of pulling in the full ingestion TreeSitterParser\n    which depends on Neo4j/jobs.\n    \"\"\"\n    # Return from cache if already loaded\n    if lang in _parser_cache:\n        return _parser_cache[lang]\n\n    # Unknown language → skip\n    if lang not in _LANG_PARSER_REGISTRY:\n        return None\n\n    module_path, class_name = _LANG_PARSER_REGISTRY[lang]\n    try:\n        from common.tree_sitter_manager import get_language_safe, create_parser\n\n        # Build the adapter the language parser constructors expect\n        class _Adapter:\n            pass\n        adapter = _Adapter()\n        adapter.language_name = lang\n        adapter.language = get_language_safe(lang)   # tree-sitter Language object\n        adapter.parser = create_parser(lang)         # tree-sitter Parser object\n\n        # Dynamically import the language-specific parser class\n        module = importlib.import_module(module_path)\n        parser_class = getattr(module, class_name)\n\n        # Instantiate and configure\n        parser_instance = parser_class(adapter)\n        parser_instance.index_source = False  # we don't need source code stored on each node\n\n        _parser_cache[lang] = parser_instance\n        return parser_instance\n    except Exception as e:\n        logger.warning(f\"Failed to load parser for {lang}: {e}\")\n        return None\n\n\n# ==========================================================================\n# Diff parsing helpers\n# ==========================================================================\n\ndef _extract_added_source_by_file(diff_text: str) -> Dict[str, str]:\n    \"\"\"\n    Walk through a unified diff and collect added lines per file.\n\n    Returns {\"app/services/foo.py\": \"from x import y\\\\ndef bar(): ...\"}\n\n    Only lines starting with '+' (but not '+++ b/...') are collected,\n    with the leading '+' stripped so tree-sitter can parse the source.\n    \"\"\"\n    files: Dict[str, List[str]] = {}\n    current_file = None\n\n    for line in diff_text.splitlines():\n        # \"+++ b/path/to/file\" marks the start of a new file in the diff\n        if line.startswith(\"+++ b/\"):\n            current_file = line[6:]  # strip \"+++ b/\" prefix\n            if current_file not in files:\n                files[current_file] = []\n        # Lines starting with \"+\" are added lines (skip the \"+++ b/\" header)\n        elif line.startswith(\"+\") and not line.startswith(\"+++\") and current_file:\n            files[current_file].append(line[1:])  # strip the leading \"+\"\n\n    # Join lines into a single source string per file, skip empty files\n    return {f: \"\\n\".join(lines) for f, lines in files.items() if lines}\n\n\n# ==========================================================================\n# Import & symbol extraction from diff (tree-sitter powered)\n# ==========================================================================\n\ndef extract_imports_from_diff(diff_text: str) -> List[Dict]:\n    \"\"\"\n    Extract imports from the added lines of a diff using tree-sitter.\n\n    Uses the same language parsers as the ingestion service, so the import\n    format matches what the graph stores:\n      Python:     {\"name\": \"foo\", \"full_import_name\": \"bar.foo\", ...}\n      JavaScript: {\"name\": \"useState\", \"source\": \"react\", ...}\n    \"\"\"\n    all_imports: List[Dict] = []\n\n    # Group added source by file path\n    added_source = _extract_added_source_by_file(diff_text)\n\n    for file_path, source in added_source.items():\n        # Determine language from file extension\n        ext = Path(file_path).suffix.lower()\n        lang = _EXT_TO_LANG.get(ext)\n        if not lang:\n            continue\n\n        # Load the language-specific parser\n        parser = _get_lang_parser(lang)\n        if not parser or not hasattr(parser, \"_find_imports\"):\n            continue\n\n        try:\n            # Parse the added source into a tree-sitter AST\n            tree = parser.parser.parse(source.encode(\"utf-8\"))\n            # Use the parser's own _find_imports to extract structured import data\n            file_imports = parser._find_imports(tree.root_node)\n            all_imports.extend(file_imports)\n        except Exception as e:\n            logger.warning(f\"Import extraction failed for {file_path}: {e}\")\n\n    return all_imports\n\n\ndef extract_symbols_from_diff(diff_text: str) -> Tuple[Set[str], Set[str]]:\n    \"\"\"\n    Extract function and class names defined in the added lines of a diff.\n\n    Returns (function_names, class_names) as sets of strings.\n    \"\"\"\n    functions: Set[str] = set()\n    classes: Set[str] = set()\n\n    added_source = _extract_added_source_by_file(diff_text)\n\n    print('----- The Added source for the diff -----')\n    print(added_source)\n    print('-----------------------------------------------------')\n\n    for file_path, source in added_source.items():\n        #get the file extension\n        ext = Path(file_path).suffix.lower()\n\n        #get the language from the file extension\n        lang = _EXT_TO_LANG.get(ext)\n        if not lang:\n            continue\n\n        #get the language-specific parser\n        parser = _get_lang_parser(lang)\n        if not parser:\n            continue\n\n        try:\n            #parse the added source into a tree-sitter AST\n            tree = parser.parser.parse(source.encode(\"utf-8\"))\n            root = tree.root_node\n\n            # Extract function definitions\n            if hasattr(parser, \"_find_functions\"):\n                for func in parser._find_functions(root):\n                    functions.add(func.get(\"name\", \"\"))\n\n            # Extract class definitions\n            if hasattr(parser, \"_find_classes\"):\n                for cls in parser._find_classes(root):\n                    classes.add(cls.get(\"name\", \"\"))\n        except Exception as e:\n            logger.warning(f\"Symbol extraction failed for {file_path}: {e}\")\n\n    return functions, classes\n\n\n# ==========================================================================\n# Graph-based resolution\n# ==========================================================================\n\ndef _resolve_imports_from_graph(\n    neo4j_client: Neo4jClient, imports: List[Dict], repo_id: str,\n) -> List[Dict]:\n    \"\"\"\n    For each imported name, search the Neo4j graph for its definition.\n\n    Queries directly with repo filtering (WHERE n.repo = $repo) so results\n    are scoped to the PR's repository. Searches Function, Class, and Variable\n    nodes by exact name match.\n\n    Returns a list of resolved import dicts with source code from the graph.\n    \"\"\"\n    resolved = []\n    seen = set()  # avoid looking up the same name twice\n\n    with neo4j_client.driver.session() as session:\n        for imp in imports:\n            name = imp.get(\"name\", \"\")\n\n            # Skip empty, already-seen, or wildcard imports\n            if not name or name in seen or name == \"*\":\n                continue\n            seen.add(name)\n\n            # full_import_name for Python (\"app.services.foo.bar\"),\n            # source for JS/TS (\"react\", \"./utils\")\n            module = imp.get(\"full_import_name\", imp.get(\"source\", \"\"))\n\n            try:\n                # Search for the symbol in the graph, scoped to this repo\n                result = session.run(\"\"\"\n                    MATCH (n)\n                    WHERE (n:Function OR n:Class OR n:Variable)\n                      AND n.name = $name\n                      AND n.repo = $repo\n                    RETURN n.name AS name, n.path AS path,\n                           n.line_number AS line_number,\n                           n.source AS source, n.docstring AS docstring,\n                           labels(n) AS labels\n                    LIMIT 1\n                \"\"\", name=name, repo=repo_id)\n\n                records = result.data()\n                if not records:\n                    continue  # symbol not in graph (external dep or not ingested)\n\n                r = records[0]\n                labels = r.get(\"labels\", [])\n                node_type = \"class\" if \"Class\" in labels else \"function\"\n\n                resolved.append({\n                    \"name\": name,\n                    \"module\": module,\n                    \"type\": node_type,\n                    \"source\": r.get(\"source\", \"\"),\n                    \"path\": r.get(\"path\", \"\"),\n                    \"line\": r.get(\"line_number\"),\n                    \"docstring\": r.get(\"docstring\", \"\"),\n                })\n            except Exception as e:\n                logger.warning(f\"Failed to resolve import {name}: {e}\")\n\n    return resolved\n\n\ndef _find_callers(code_finder, symbol_names: Set[str]) -> List[Dict]:\n    \"\"\"\n    For each changed symbol, find functions that call it (impact analysis).\n\n    Uses CodeFinder.who_calls_function which queries CALLS relationships\n    in the graph.  Returns at most 10 callers per symbol.\n    \"\"\"\n    callers = []\n\n    for name in symbol_names:\n        try:\n            results = code_finder.who_calls_function(name)\n            if not results:\n                continue\n\n            # Extract caller info, limit to 10 per symbol\n            caller_list = [\n                {\n                    \"name\": c.get(\"caller_function\"),\n                    \"path\": c.get(\"caller_file_path\"),\n                    \"line\": c.get(\"caller_line_number\"),\n                }\n                for c in results\n                if c.get(\"caller_function\")\n            ][:10]\n\n            if caller_list:\n                callers.append({\"symbol\": name, \"callers\": caller_list})\n        except Exception as e:\n            logger.warning(f\"Failed to find callers for {name}: {e}\")\n\n    return callers\n\n\n# ==========================================================================\n# Context formatting (for LLM agents)\n# ==========================================================================\n\ndef _build_agent_context(\n    import_context: List[Dict],\n    caller_context: List[Dict],\n    graph_section: str,\n) -> str:\n    \"\"\"\n    Build the markdown context string that gets sent to the LLM review agents.\n\n    Combines:\n      - Resolved import source code from the graph\n      - Impact analysis (who calls the changed code)\n      - Graph context (symbols in changed line ranges)\n    \"\"\"\n    parts = []\n\n    # Section 1: Source code of imported dependencies\n    if import_context:\n        parts.append(\"## Imported Dependencies (source from graph)\")\n        for imp in import_context:\n            source = imp.get(\"source\", \"\")\n            if source:\n                parts.append(f\"### `{imp['name']}` from `{imp['module']}` ({imp.get('path', '')})\")\n                parts.append(f\"```python\\n{source}\\n```\")\n            else:\n                parts.append(f\"- `{imp['name']}` from `{imp['module']}` (source not in graph)\")\n        parts.append(\"\")\n\n    # Section 2: Callers of changed functions\n    if caller_context:\n        parts.append(\"## Impact Analysis (who calls the changed code)\")\n        for entry in caller_context:\n            caller_names = \", \".join(\n                f\"`{c['name']}` in `{c.get('path', '?')}`\" for c in entry[\"callers\"]\n            )\n            parts.append(f\"- `{entry['symbol']}` is called by: {caller_names}\")\n        parts.append(\"\")\n\n    # Section 3: Graph context (symbols overlapping changed lines)\n    if graph_section and graph_section != \"No graph context available.\":\n        parts.append(\"## Graph Context\")\n        parts.append(graph_section)\n        parts.append(\"\")\n\n    return \"\\n\".join(parts) if parts else \"No additional context available.\"\n\n\n# ==========================================================================\n# Debug dump (writes context to output/<pr_number>/context_<timestamp>.md)\n# ==========================================================================\n\ndef _dump_context_to_file(\n    owner: str, repo: str, pr_number: int, diff_text: str,\n    agent_context: str, raw_imports: List[Dict], resolved_imports: List[Dict],\n    caller_context: List[Dict], graph_symbols: list,\n    files_changed: list, changed_symbol_names: list, risk_level: str,\n) -> None:\n    \"\"\"Save all gathered context to a markdown file for debugging/inspection.\"\"\"\n    try:\n        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n        output_dir = Path(\"output\") / str(pr_number)\n        output_dir.mkdir(parents=True, exist_ok=True)\n        context_file = output_dir / f\"context_{timestamp}.md\"\n\n        total_callers = sum(len(c[\"callers\"]) for c in caller_context)\n\n        lines = [\n            f\"# PR Review Context — {owner}/{repo}#{pr_number}\",\n            f\"**Generated at:** {datetime.now().isoformat()}\",\n            f\"**Risk level:** {risk_level}\",\n            \"\",\n            # What files were touched\n            \"## Files Changed\",\n            *[f\"- `{f}`\" for f in files_changed],\n            \"\",\n            # Functions/classes defined in the added lines\n            \"## Changed Symbols (from diff)\",\n            *[f\"- `{s}`\" for s in changed_symbol_names],\n            \"\",\n            # All imports found in the added lines (tree-sitter extraction)\n            f\"## Extracted Imports from Diff ({len(raw_imports)})\",\n            *[f\"- `{imp.get('name', '?')}` (full: `{imp.get('full_import_name', imp.get('source', '?'))}`)\"\n              for imp in raw_imports],\n            \"\",\n            # Which of those imports we found in the graph (with source code)\n            f\"## Resolved Imports ({len(resolved_imports)}/{len(raw_imports)} resolved from graph)\",\n            *[f\"- `{imp['name']}` from `{imp['module']}` → `{imp.get('path', 'not found')}`\"\n              for imp in resolved_imports],\n            \"\",\n            # Functions that call the changed symbols\n            f\"## Caller Impact ({total_callers} callers)\",\n            *[f\"- `{c['symbol']}` called by: \"\n              + \", \".join(f\"`{caller['name']}`\" for caller in c[\"callers\"])\n              for c in caller_context],\n            \"\",\n            # Symbols from the graph that overlap with changed line ranges\n            f\"## Graph Symbols ({len(graph_symbols)})\",\n            *[f\"- `{s.get('name', '')}` ({s.get('type', 'unknown')}) in `{s.get('file_path', '')}`\"\n              for s in graph_symbols],\n            \"\",\n            # The full context string that gets sent to the LLM agents\n            \"## Context Sent to Agents\",\n            agent_context,\n            \"\",\n            # Raw diff for reference\n            \"## Diff\",\n            \"```diff\",\n            diff_text,\n            \"```\",\n        ]\n\n        context_file.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n        logger.info(f\"Review context saved to {context_file}\")\n    except Exception as e:\n        logger.warning(f\"Failed to dump review context: {e}\")\n\n\n# ==========================================================================\n# Main pipeline\n# ==========================================================================\n\nasync def execute_pr_review(owner: str, repo: str, pr_number: int) -> None:\n    \"\"\"\n    Full PR review pipeline.\n\n    Steps:\n      1. Fetch the PR diff from GitHub\n      2. Parse the diff into changed file paths and line ranges\n      3. Extract imports and symbol definitions from added lines (tree-sitter)\n      4. Connect to Neo4j and resolve context:\n         a. Find graph symbols overlapping changed line ranges\n         b. Look up imported symbols in the graph (get source code)\n         c. Find callers of changed functions (impact analysis)\n      5. Build risk level based on scope of changes\n      6. Format everything into context for the LLM agents\n      7. Dump context to file for debugging\n      8. Run multi-agent review (bug-hunter + security-auditor)\n      9. Format and post the review comment on the PR\n    \"\"\"\n    try:\n        logger.info(f\"Starting review pipeline for {owner}/{repo}#{pr_number}\")\n        repo_id = f\"{owner}/{repo}\"  # matches the repo identifier stored in the graph\n\n        # ── Step 1: Fetch diff from GitHub ──────────────────────────────\n        gh = GitHubClient()\n        diff_text = await gh.get_pr_diff(owner, repo, pr_number)\n        if not diff_text:\n            logger.warning(\"Empty diff, skipping review\")\n            return\n        logger.info(f\"Fetched diff ({len(diff_text)} chars)\")\n\n        # ── Step 2: Parse diff structure ────────────────────────────────\n        # Returns list of {\"file_path\", \"start_line\", \"end_line\"} hunks\n        changes = parse_unified_diff(diff_text)\n        files_changed = list({c[\"file_path\"] for c in changes})\n        logger.info(f\"Parsed {len(changes)} hunks across {len(files_changed)} files\")\n\n        # ── Step 3: Extract imports & symbols from added lines ──────────\n        # Uses the same tree-sitter language parsers as the ingestion service\n        diff_imports = extract_imports_from_diff(diff_text)\n\n        print('-----------------------------------------------------')\n        print(diff_imports)\n        print('-----------------------------------------------------')\n\n        diff_functions, diff_classes = extract_symbols_from_diff(diff_text)\n\n        print('-----------------------------------------------------')\n        print(diff_functions)\n        print(diff_classes)\n        print('-----------------------------------------------------')\n\n    \n        all_diff_symbols = diff_functions | diff_classes\n        logger.info(\n            f\"Diff extraction: {len(diff_imports)} imports, \"\n            f\"{len(diff_functions)} functions, {len(diff_classes)} classes\"\n        )\n\n        # ── Step 4: Connect to Neo4j and resolve context ────────────────\n        neo4j = Neo4jClient(\n            uri=os.environ.get(\"NEO4J_URI\", \"\"),\n            user=os.environ.get(\"NEO4J_USERNAME\", \"neo4j\"),\n            password=os.environ.get(\"NEO4J_PASSWORD\", \"\"),\n            database=os.environ.get(\"NEO4J_DATABASE\", \"neo4j\"),\n        )\n        query_service = CodeQueryService(neo4j)\n        from api.services.code_search import CodeFinder\n        code_finder = CodeFinder(neo4j)\n\n        # 4a. Graph symbols overlapping the changed line ranges\n        graph_context = query_service.get_diff_context_enhanced(repo_id, changes)\n        graph_symbols = graph_context.get(\"affected_symbols\", [])\n        graph_section = build_graph_context_section(graph_context)\n\n        # 4b. Look up each imported name in the graph (scoped to this repo)\n        resolved_imports = _resolve_imports_from_graph(neo4j, diff_imports, repo_id)\n        logger.info(f\"Resolved {len(resolved_imports)}/{len(diff_imports)} imports from graph\")\n\n        # 4c. Find callers of changed functions (impact analysis)\n        # Merge symbols from diff extraction + graph overlap\n        all_symbol_names = all_diff_symbols | {s.get(\"name\", \"\") for s in graph_symbols}\n        caller_context = _find_callers(code_finder, all_symbol_names)\n        total_callers = sum(len(c[\"callers\"]) for c in caller_context)\n        logger.info(f\"Found {total_callers} callers across {len(caller_context)} symbols\")\n\n        # ── Step 5: Determine risk level ────────────────────────────────\n        changed_symbol_names = list(all_symbol_names)\n        if total_callers > 10 or len(all_symbol_names) > 5:\n            risk_level = \"high\"\n        elif total_callers > 3 or len(all_symbol_names) > 2:\n            risk_level = \"medium\"\n        else:\n            risk_level = \"low\"\n\n        # ── Step 6: Build context for LLM agents ───────────────────────\n        agent_context = _build_agent_context(\n            resolved_imports, caller_context, graph_section\n        )\n\n        # ── Step 7: Dump context to file for debugging ──────────────────\n        _dump_context_to_file(\n            owner, repo, pr_number, diff_text,\n            agent_context, diff_imports, resolved_imports, caller_context,\n            graph_symbols, files_changed, changed_symbol_names, risk_level,\n        )\n\n        # ── Step 8: Run multi-agent review ──────────────────────────────\n        review_results = await run_review(diff_text, agent_context, repo_id, pr_number)\n        logger.info(f\"Review complete: {len(review_results.issues)} issues found\")\n\n        # ── Step 9: Format and post comment ─────────────────────────────\n        context_data = ContextData(\n            files_changed=files_changed,\n            modified_symbols=changed_symbol_names,\n            total_callers=total_callers,\n            risk_level=risk_level,\n        )\n        final_comment = format_github_comment(review_results, context_data, pr_number)\n        await gh.post_comment(owner, repo, pr_number, final_comment)\n        logger.info(f\"Posted review comment on {owner}/{repo}#{pr_number}\")\n\n    except Exception:\n        logger.error(f\"Review pipeline failed: {traceback.format_exc()}\")\n","numLines":600,"startLine":1,"totalLines":600}},"sourceToolAssistantUUID":"bad2c35d-5174-4251-90cd-448db8a63749"}
{"parentUuid":"caf6a7cc-7daa-4b40-a48a-48c697325d74","isSidechain":false,"userType":"external","cwd":"/Users/skmabudalam/Documents/BugViper","sessionId":"0f25fa6f-5425-4cfc-8c2e-2b3c7eda02c7","version":"2.1.39","gitBranch":"review-pipeline-improvements","slug":"immutable-launching-music","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_011UFUca87YBzuhqvycUn4YR","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_017uxvD9CBmMWfdbcLEbgpD1","name":"Read","input":{"file_path":"/Users/skmabudalam/Documents/BugViper/deepagent/agent/review_pipeline.py"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":80,"cache_read_input_tokens":21948,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":80},"output_tokens":0,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY5fvwdsNmDiBBBPakTGn","type":"assistant","uuid":"4ad74345-3974-4e23-824e-b179f6bb1772","timestamp":"2026-02-13T07:27:09.040Z"}
{"parentUuid":"4ad74345-3974-4e23-824e-b179f6bb1772","isSidechain":false,"userType":"external","cwd":"/Users/skmabudalam/Documents/BugViper","sessionId":"0f25fa6f-5425-4cfc-8c2e-2b3c7eda02c7","version":"2.1.39","gitBranch":"review-pipeline-improvements","slug":"immutable-launching-music","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Read","command":"callback"},"parentToolUseID":"toolu_017uxvD9CBmMWfdbcLEbgpD1","toolUseID":"toolu_017uxvD9CBmMWfdbcLEbgpD1","timestamp":"2026-02-13T07:27:09.044Z","uuid":"982da3c7-f6f0-4438-8d7f-d9db96b4114a"}
{"parentUuid":"4ad74345-3974-4e23-824e-b179f6bb1772","isSidechain":false,"userType":"external","cwd":"/Users/skmabudalam/Documents/BugViper","sessionId":"0f25fa6f-5425-4cfc-8c2e-2b3c7eda02c7","version":"2.1.39","gitBranch":"review-pipeline-improvements","slug":"immutable-launching-music","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_017uxvD9CBmMWfdbcLEbgpD1","type":"tool_result","content":"     1→\"\"\"Multi-agent PR review pipeline using pydantic-ai.\"\"\"\n     2→\n     3→import asyncio\n     4→import logging\n     5→import os\n     6→\n     7→from deepagent.config import config\n     8→\n     9→# Ensure OPENROUTER_API_KEY is in the process env so pydantic-ai's\n    10→# OpenRouterProvider can pick it up (our config reads .env only).\n    11→os.environ.setdefault(\"OPENROUTER_API_KEY\", config.openrouter_api_key)\n    12→\n    13→from pydantic_ai import Agent  # noqa: E402\n    14→from deepagent.models.agent_schemas import AgentFindings, Issue, ReviewResults\n    15→from deepagent.prompts import BUG_HUNTER_PROMPT, SECURITY_AUDITOR_PROMPT\n    16→\n    17→logger = logging.getLogger(__name__)\n    18→\n    19→MODEL = f\"openrouter:{config.review_model}\"\n    20→\n    21→bug_hunter = Agent(\n    22→    MODEL,\n    23→    system_prompt=BUG_HUNTER_PROMPT,\n    24→    output_type=AgentFindings,\n    25→    name=\"bug-hunter\",\n    26→)\n    27→\n    28→security_auditor = Agent(\n    29→    MODEL,\n    30→    system_prompt=SECURITY_AUDITOR_PROMPT,\n    31→    output_type=AgentFindings,\n    32→    name=\"security-auditor\",\n    33→)\n    34→\n    35→# Logfire instrumentation\n    36→try:\n    37→    import logfire\n    38→\n    39→    if config.enable_logfire and config.logfire_token:\n    40→        logfire.configure(token=config.logfire_token)\n    41→        logfire.instrument_pydantic_ai(bug_hunter)\n    42→        logfire.instrument_pydantic_ai(security_auditor)\n    43→except ImportError:\n    44→    pass\n    45→\n    46→\n    47→def _build_user_prompt(diff_text: str, graph_context_section: str, repo_id: str, pr_number: int) -> str:\n    48→    return (\n    49→        f\"## PR #{pr_number} in {repo_id}\\n\\n\"\n    50→        f\"### Diff\\n```diff\\n{diff_text}\\n```\\n\\n\"\n    51→        f\"### Dependency Graph Context\\n{graph_context_section}\"\n    52→    )\n    53→\n    54→\n    55→def _dedup_issues(issues: list[Issue]) -> list[Issue]:\n    56→    \"\"\"Remove duplicate issues by (file, line_start, title).\"\"\"\n    57→    seen: set[tuple[str, int, str]] = set()\n    58→    deduped: list[Issue] = []\n    59→    for issue in issues:\n    60→        key = (issue.file, issue.line_start, issue.title)\n    61→        if key not in seen:\n    62→            seen.add(key)\n    63→            deduped.append(issue)\n    64→    return deduped\n    65→\n    66→\n    67→async def run_review(\n    68→    diff_text: str,\n    69→    graph_context_section: str,\n    70→    repo_id: str,\n    71→    pr_number: int,\n    72→) -> ReviewResults:\n    73→    \"\"\"Run bug-hunter and security-auditor in parallel, merge results.\"\"\"\n    74→\n    75→    prompt = _build_user_prompt(diff_text, graph_context_section, repo_id, pr_number)\n    76→\n    77→    logger.info(f\"Starting multi-agent review for {repo_id}#{pr_number}\")\n    78→\n    79→    bug_result, sec_result = await asyncio.gather(\n    80→        bug_hunter.run(prompt),\n    81→        security_auditor.run(prompt),\n    82→    )\n    83→\n    84→    bug_findings: AgentFindings = bug_result.output\n    85→    sec_findings: AgentFindings = sec_result.output\n    86→\n    87→    logger.info(\n    88→        f\"bug-hunter: {len(bug_findings.issues)} issues, \"\n    89→        f\"security-auditor: {len(sec_findings.issues)} issues\"\n    90→    )\n    91→\n    92→    all_issues = _dedup_issues(bug_findings.issues + sec_findings.issues)\n    93→    all_positives = list(dict.fromkeys(\n    94→        bug_findings.positive_findings + sec_findings.positive_findings\n    95→    ))\n    96→\n    97→    issue_count = len(all_issues)\n    98→    critical = sum(1 for i in all_issues if i.severity == \"critical\")\n    99→    high = sum(1 for i in all_issues if i.severity == \"high\")\n   100→\n   101→    if issue_count == 0:\n   102→        summary = \"No significant issues found. The code looks good.\"\n   103→    else:\n   104→        summary = (\n   105→            f\"Found {issue_count} issue(s) \"\n   106→            f\"({critical} critical, {high} high). \"\n   107→            \"Review the details below.\"\n   108→        )\n   109→\n   110→    return ReviewResults(\n   111→        summary=summary,\n   112→        issues=all_issues,\n   113→        positive_findings=all_positives,\n   114→    )\n   115→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"59d2375c-4357-451e-8028-b012e04f9f48","timestamp":"2026-02-13T07:27:09.043Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/skmabudalam/Documents/BugViper/deepagent/agent/review_pipeline.py","content":"\"\"\"Multi-agent PR review pipeline using pydantic-ai.\"\"\"\n\nimport asyncio\nimport logging\nimport os\n\nfrom deepagent.config import config\n\n# Ensure OPENROUTER_API_KEY is in the process env so pydantic-ai's\n# OpenRouterProvider can pick it up (our config reads .env only).\nos.environ.setdefault(\"OPENROUTER_API_KEY\", config.openrouter_api_key)\n\nfrom pydantic_ai import Agent  # noqa: E402\nfrom deepagent.models.agent_schemas import AgentFindings, Issue, ReviewResults\nfrom deepagent.prompts import BUG_HUNTER_PROMPT, SECURITY_AUDITOR_PROMPT\n\nlogger = logging.getLogger(__name__)\n\nMODEL = f\"openrouter:{config.review_model}\"\n\nbug_hunter = Agent(\n    MODEL,\n    system_prompt=BUG_HUNTER_PROMPT,\n    output_type=AgentFindings,\n    name=\"bug-hunter\",\n)\n\nsecurity_auditor = Agent(\n    MODEL,\n    system_prompt=SECURITY_AUDITOR_PROMPT,\n    output_type=AgentFindings,\n    name=\"security-auditor\",\n)\n\n# Logfire instrumentation\ntry:\n    import logfire\n\n    if config.enable_logfire and config.logfire_token:\n        logfire.configure(token=config.logfire_token)\n        logfire.instrument_pydantic_ai(bug_hunter)\n        logfire.instrument_pydantic_ai(security_auditor)\nexcept ImportError:\n    pass\n\n\ndef _build_user_prompt(diff_text: str, graph_context_section: str, repo_id: str, pr_number: int) -> str:\n    return (\n        f\"## PR #{pr_number} in {repo_id}\\n\\n\"\n        f\"### Diff\\n```diff\\n{diff_text}\\n```\\n\\n\"\n        f\"### Dependency Graph Context\\n{graph_context_section}\"\n    )\n\n\ndef _dedup_issues(issues: list[Issue]) -> list[Issue]:\n    \"\"\"Remove duplicate issues by (file, line_start, title).\"\"\"\n    seen: set[tuple[str, int, str]] = set()\n    deduped: list[Issue] = []\n    for issue in issues:\n        key = (issue.file, issue.line_start, issue.title)\n        if key not in seen:\n            seen.add(key)\n            deduped.append(issue)\n    return deduped\n\n\nasync def run_review(\n    diff_text: str,\n    graph_context_section: str,\n    repo_id: str,\n    pr_number: int,\n) -> ReviewResults:\n    \"\"\"Run bug-hunter and security-auditor in parallel, merge results.\"\"\"\n\n    prompt = _build_user_prompt(diff_text, graph_context_section, repo_id, pr_number)\n\n    logger.info(f\"Starting multi-agent review for {repo_id}#{pr_number}\")\n\n    bug_result, sec_result = await asyncio.gather(\n        bug_hunter.run(prompt),\n        security_auditor.run(prompt),\n    )\n\n    bug_findings: AgentFindings = bug_result.output\n    sec_findings: AgentFindings = sec_result.output\n\n    logger.info(\n        f\"bug-hunter: {len(bug_findings.issues)} issues, \"\n        f\"security-auditor: {len(sec_findings.issues)} issues\"\n    )\n\n    all_issues = _dedup_issues(bug_findings.issues + sec_findings.issues)\n    all_positives = list(dict.fromkeys(\n        bug_findings.positive_findings + sec_findings.positive_findings\n    ))\n\n    issue_count = len(all_issues)\n    critical = sum(1 for i in all_issues if i.severity == \"critical\")\n    high = sum(1 for i in all_issues if i.severity == \"high\")\n\n    if issue_count == 0:\n        summary = \"No significant issues found. The code looks good.\"\n    else:\n        summary = (\n            f\"Found {issue_count} issue(s) \"\n            f\"({critical} critical, {high} high). \"\n            \"Review the details below.\"\n        )\n\n    return ReviewResults(\n        summary=summary,\n        issues=all_issues,\n        positive_findings=all_positives,\n    )\n","numLines":115,"startLine":1,"totalLines":115}},"sourceToolAssistantUUID":"4ad74345-3974-4e23-824e-b179f6bb1772"}
{"parentUuid":"59d2375c-4357-451e-8028-b012e04f9f48","isSidechain":false,"userType":"external","cwd":"/Users/skmabudalam/Documents/BugViper","sessionId":"0f25fa6f-5425-4cfc-8c2e-2b3c7eda02c7","version":"2.1.39","gitBranch":"review-pipeline-improvements","slug":"immutable-launching-music","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_016ohCCJxX55rxAJobXRA4pe","type":"message","role":"assistant","content":[{"type":"thinking","thinking":"The user wants to:\n1. Create a new folder called `review-{current_time}/` each time `execute_pr_review` is called\n2. For each step in `execute_pr_review`, store the results in `.md` files inside that folder\n3. Skip calling the LLM (skip step 8 and 9)\n\nI need to modify `api/services/review_service.py` to:\n1. Create the folder at the start of `execute_pr_review`\n2. After each step, write a `.md` file with the results\n3. Skip calling `run_review` (the LLM call) and `post_comment`\n\n\n4. I'm mapping out the output files for each step: `01_diff.md` for the raw diff, `02_parsed_diff.md` for the parsed changes, `03_extracted_symbols.md` for the imports and functions, then `04_graph_context.md`, `05_resolved_imports.md`, and `06_caller_context.md` for the context gathering steps.\n\n5. The existing dump function combines everything at once, but I need to write individual files after each step instead. I'll create a helper function to write each step's results to its own markdown file.\n\n6. I should create the review directory with a timestamp at the start of `execute_pr_review`, then call the step-writing function after each major operation completes.\n\n7. For skipping the LLM steps, I'll just comment out or bypass steps 8 and 9 in the service code.\n\nNow I'm modifying `review_service.py`...","signature":"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"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":10296,"cache_read_input_tokens":22028,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":10296},"output_tokens":9,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY5fwEJUTbvoGwmMaDChq","type":"assistant","uuid":"e6f60237-ce8f-4685-b98e-9d03501c5cb2","timestamp":"2026-02-13T07:27:21.844Z"}
{"parentUuid":"e6f60237-ce8f-4685-b98e-9d03501c5cb2","isSidechain":false,"userType":"external","cwd":"/Users/skmabudalam/Documents/BugViper","sessionId":"0f25fa6f-5425-4cfc-8c2e-2b3c7eda02c7","version":"2.1.39","gitBranch":"review-pipeline-improvements","slug":"immutable-launching-music","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_016ohCCJxX55rxAJobXRA4pe","type":"message","role":"assistant","content":[{"type":"text","text":"Now I have a clear picture. I'll modify `execute_pr_review` to:\n1. Create `review-{timestamp}/` at the start\n2. Write a `.md` file after each step\n3. Skip the LLM call (step 8) and the comment post (step 9)"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":10296,"cache_read_input_tokens":22028,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":10296},"output_tokens":9,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY5fwEJUTbvoGwmMaDChq","type":"assistant","uuid":"8296b7c1-55fa-4398-8b41-fc8696d819c6","timestamp":"2026-02-13T07:27:22.109Z"}
{"parentUuid":"8296b7c1-55fa-4398-8b41-fc8696d819c6","isSidechain":false,"userType":"external","cwd":"/Users/skmabudalam/Documents/BugViper","sessionId":"0f25fa6f-5425-4cfc-8c2e-2b3c7eda02c7","version":"2.1.39","gitBranch":"review-pipeline-improvements","slug":"immutable-launching-music","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_016ohCCJxX55rxAJobXRA4pe","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01XoJzMEpTtZwVKHk5cDU1QA","name":"Edit","input":{"replace_all":false,"file_path":"/Users/skmabudalam/Documents/BugViper/api/services/review_service.py","old_string":"# ==========================================================================\n# Debug dump (writes context to output/<pr_number>/context_<timestamp>.md)\n# ==========================================================================\n\ndef _dump_context_to_file(\n    owner: str, repo: str, pr_number: int, diff_text: str,\n    agent_context: str, raw_imports: List[Dict], resolved_imports: List[Dict],\n    caller_context: List[Dict], graph_symbols: list,\n    files_changed: list, changed_symbol_names: list, risk_level: str,\n) -> None:\n    \"\"\"Save all gathered context to a markdown file for debugging/inspection.\"\"\"\n    try:\n        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n        output_dir = Path(\"output\") / str(pr_number)\n        output_dir.mkdir(parents=True, exist_ok=True)\n        context_file = output_dir / f\"context_{timestamp}.md\"\n\n        total_callers = sum(len(c[\"callers\"]) for c in caller_context)\n\n        lines = [\n            f\"# PR Review Context — {owner}/{repo}#{pr_number}\",\n            f\"**Generated at:** {datetime.now().isoformat()}\",\n            f\"**Risk level:** {risk_level}\",\n            \"\",\n            # What files were touched\n            \"## Files Changed\",\n            *[f\"- `{f}`\" for f in files_changed],\n            \"\",\n            # Functions/classes defined in the added lines\n            \"## Changed Symbols (from diff)\",\n            *[f\"- `{s}`\" for s in changed_symbol_names],\n            \"\",\n            # All imports found in the added lines (tree-sitter extraction)\n            f\"## Extracted Imports from Diff ({len(raw_imports)})\",\n            *[f\"- `{imp.get('name', '?')}` (full: `{imp.get('full_import_name', imp.get('source', '?'))}`)\"\n              for imp in raw_imports],\n            \"\",\n            # Which of those imports we found in the graph (with source code)\n            f\"## Resolved Imports ({len(resolved_imports)}/{len(raw_imports)} resolved from graph)\",\n            *[f\"- `{imp['name']}` from `{imp['module']}` → `{imp.get('path', 'not found')}`\"\n              for imp in resolved_imports],\n            \"\",\n            # Functions that call the changed symbols\n            f\"## Caller Impact ({total_callers} callers)\",\n            *[f\"- `{c['symbol']}` called by: \"\n              + \", \".join(f\"`{caller['name']}`\" for caller in c[\"callers\"])\n              for c in caller_context],\n            \"\",\n            # Symbols from the graph that overlap with changed line ranges\n            f\"## Graph Symbols ({len(graph_symbols)})\",\n            *[f\"- `{s.get('name', '')}` ({s.get('type', 'unknown')}) in `{s.get('file_path', '')}`\"\n              for s in graph_symbols],\n            \"\",\n            # The full context string that gets sent to the LLM agents\n            \"## Context Sent to Agents\",\n            agent_context,\n            \"\",\n            # Raw diff for reference\n            \"## Diff\",\n            \"```diff\",\n            diff_text,\n            \"```\",\n        ]\n\n        context_file.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n        logger.info(f\"Review context saved to {context_file}\")\n    except Exception as e:\n        logger.warning(f\"Failed to dump review context: {e}\")","new_string":"# ==========================================================================\n# Per-step debug dump helpers\n# ==========================================================================\n\ndef _make_review_dir(owner: str, repo: str, pr_number: int) -> Path:\n    \"\"\"Create and return a timestamped folder for this review run.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n    review_dir = Path(\"output\") / f\"review-{timestamp}\"\n    review_dir.mkdir(parents=True, exist_ok=True)\n    logger.info(f\"Review debug dir: {review_dir}\")\n    return review_dir\n\n\ndef _write_step(review_dir: Path, filename: str, content: str) -> None:\n    \"\"\"Write a single step's debug output to <review_dir>/<filename>.md.\"\"\"\n    try:\n        (review_dir / filename).write_text(content, encoding=\"utf-8\")\n    except Exception as e:\n        logger.warning(f\"Failed to write {filename}: {e}\")"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":10296,"cache_read_input_tokens":22028,"output_tokens":1842,"server_tool_use":{"web_search_requests":0,"web_fetch_requests":0},"service_tier":"standard","cache_creation":{"ephemeral_1h_input_tokens":10296,"ephemeral_5m_input_tokens":0},"inference_geo":"","iterations":[]}},"requestId":"req_011CY5fwEJUTbvoGwmMaDChq","type":"assistant","uuid":"dcb9782c-6280-46c9-828d-e4465b3cd525","timestamp":"2026-02-13T07:27:38.729Z"}
{"type":"file-history-snapshot","messageId":"dcb9782c-6280-46c9-828d-e4465b3cd525","snapshot":{"messageId":"849b236d-6663-4b93-be47-a871b6230169","trackedFileBackups":{"api/services/review_service.py":{"backupFileName":"79c6237359372a8f@v1","version":1,"backupTime":"2026-02-13T07:29:35.338Z"}},"timestamp":"2026-02-13T07:27:05.054Z"},"isSnapshotUpdate":true}
{"parentUuid":"dcb9782c-6280-46c9-828d-e4465b3cd525","isSidechain":false,"userType":"external","cwd":"/Users/skmabudalam/Documents/BugViper","sessionId":"0f25fa6f-5425-4cfc-8c2e-2b3c7eda02c7","version":"2.1.39","gitBranch":"review-pipeline-improvements","slug":"immutable-launching-music","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Edit","command":"callback"},"parentToolUseID":"toolu_01XoJzMEpTtZwVKHk5cDU1QA","toolUseID":"toolu_01XoJzMEpTtZwVKHk5cDU1QA","timestamp":"2026-02-13T07:29:35.341Z","uuid":"390d92b5-a45b-4629-a70c-eaa4911a1100"}
{"parentUuid":"dcb9782c-6280-46c9-828d-e4465b3cd525","isSidechain":false,"userType":"external","cwd":"/Users/skmabudalam/Documents/BugViper","sessionId":"0f25fa6f-5425-4cfc-8c2e-2b3c7eda02c7","version":"2.1.39","gitBranch":"review-pipeline-improvements","slug":"immutable-launching-music","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01XoJzMEpTtZwVKHk5cDU1QA","type":"tool_result","content":"The file /Users/skmabudalam/Documents/BugViper/api/services/review_service.py has been updated successfully."}]},"uuid":"4d539561-8b00-432a-bdaf-244c4c9998da","timestamp":"2026-02-13T07:29:35.341Z","toolUseResult":{"filePath":"/Users/skmabudalam/Documents/BugViper/api/services/review_service.py","oldString":"# ==========================================================================\n# Debug dump (writes context to output/<pr_number>/context_<timestamp>.md)\n# ==========================================================================\n\ndef _dump_context_to_file(\n    owner: str, repo: str, pr_number: int, diff_text: str,\n    agent_context: str, raw_imports: List[Dict], resolved_imports: List[Dict],\n    caller_context: List[Dict], graph_symbols: list,\n    files_changed: list, changed_symbol_names: list, risk_level: str,\n) -> None:\n    \"\"\"Save all gathered context to a markdown file for debugging/inspection.\"\"\"\n    try:\n        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n        output_dir = Path(\"output\") / str(pr_number)\n        output_dir.mkdir(parents=True, exist_ok=True)\n        context_file = output_dir / f\"context_{timestamp}.md\"\n\n        total_callers = sum(len(c[\"callers\"]) for c in caller_context)\n\n        lines = [\n            f\"# PR Review Context — {owner}/{repo}#{pr_number}\",\n            f\"**Generated at:** {datetime.now().isoformat()}\",\n            f\"**Risk level:** {risk_level}\",\n            \"\",\n            # What files were touched\n            \"## Files Changed\",\n            *[f\"- `{f}`\" for f in files_changed],\n            \"\",\n            # Functions/classes defined in the added lines\n            \"## Changed Symbols (from diff)\",\n            *[f\"- `{s}`\" for s in changed_symbol_names],\n            \"\",\n            # All imports found in the added lines (tree-sitter extraction)\n            f\"## Extracted Imports from Diff ({len(raw_imports)})\",\n            *[f\"- `{imp.get('name', '?')}` (full: `{imp.get('full_import_name', imp.get('source', '?'))}`)\"\n              for imp in raw_imports],\n            \"\",\n            # Which of those imports we found in the graph (with source code)\n            f\"## Resolved Imports ({len(resolved_imports)}/{len(raw_imports)} resolved from graph)\",\n            *[f\"- `{imp['name']}` from `{imp['module']}` → `{imp.get('path', 'not found')}`\"\n              for imp in resolved_imports],\n            \"\",\n            # Functions that call the changed symbols\n            f\"## Caller Impact ({total_callers} callers)\",\n            *[f\"- `{c['symbol']}` called by: \"\n              + \", \".join(f\"`{caller['name']}`\" for caller in c[\"callers\"])\n              for c in caller_context],\n            \"\",\n            # Symbols from the graph that overlap with changed line ranges\n            f\"## Graph Symbols ({len(graph_symbols)})\",\n            *[f\"- `{s.get('name', '')}` ({s.get('type', 'unknown')}) in `{s.get('file_path', '')}`\"\n              for s in graph_symbols],\n            \"\",\n            # The full context string that gets sent to the LLM agents\n            \"## Context Sent to Agents\",\n            agent_context,\n            \"\",\n            # Raw diff for reference\n            \"## Diff\",\n            \"```diff\",\n            diff_text,\n            \"```\",\n        ]\n\n        context_file.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n        logger.info(f\"Review context saved to {context_file}\")\n    except Exception as e:\n        logger.warning(f\"Failed to dump review context: {e}\")","newString":"# ==========================================================================\n# Per-step debug dump helpers\n# ==========================================================================\n\ndef _make_review_dir(owner: str, repo: str, pr_number: int) -> Path:\n    \"\"\"Create and return a timestamped folder for this review run.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n    review_dir = Path(\"output\") / f\"review-{timestamp}\"\n    review_dir.mkdir(parents=True, exist_ok=True)\n    logger.info(f\"Review debug dir: {review_dir}\")\n    return review_dir\n\n\ndef _write_step(review_dir: Path, filename: str, content: str) -> None:\n    \"\"\"Write a single step's debug output to <review_dir>/<filename>.md.\"\"\"\n    try:\n        (review_dir / filename).write_text(content, encoding=\"utf-8\")\n    except Exception as e:\n        logger.warning(f\"Failed to write {filename}: {e}\")","originalFile":"\"\"\"\nPR Review Service\n=================\nOrchestrates the full review pipeline:\n  1. Fetch PR diff from GitHub\n  2. Parse diff → extract added source per file\n  3. Use ingestion_service language parsers (tree-sitter) to extract imports & symbols\n  4. Resolve those imports against the Neo4j graph (same repo)\n  5. Find callers of changed functions (impact analysis)\n  6. Build context → run LLM agents → post comment\n\"\"\"\n\nimport importlib\nimport logging\nimport os\nimport traceback\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, List, Set, Tuple\n\nfrom api.utils.comment_formatter import format_github_comment\nfrom api.utils.graph_context import build_graph_context_section\nfrom db.client import Neo4jClient\nfrom db.queries import CodeQueryService\nfrom deepagent.models.agent_schemas import ContextData\nfrom deepagent.agent.review_pipeline import run_review\nfrom common.github_client import GitHubClient\nfrom common.diff_parser import parse_unified_diff\n\nlogger = logging.getLogger(__name__)\n\n\n# ==========================================================================\n# File extension → tree-sitter language name mapping (all 17 supported)\n# ==========================================================================\n_EXT_TO_LANG = {\n    \".py\": \"python\",\n    \".js\": \"javascript\", \".jsx\": \"javascript\", \".mjs\": \"javascript\", \".cjs\": \"javascript\",\n    \".ts\": \"typescript\", \".tsx\": \"typescript\",\n    \".go\": \"go\",\n    \".rs\": \"rust\",\n    \".java\": \"java\",\n    \".rb\": \"ruby\",\n    \".c\": \"c\",\n    \".cpp\": \"cpp\", \".h\": \"cpp\", \".hpp\": \"cpp\",\n    \".cs\": \"c_sharp\",\n    \".kt\": \"kotlin\",\n    \".scala\": \"scala\", \".sc\": \"scala\",\n    \".swift\": \"swift\",\n    \".php\": \"php\",\n    \".hs\": \"haskell\",\n}\n\n# ==========================================================================\n# Language parser registry\n# Maps language name → (module path, class name) for lazy import.\n# These are the SAME parsers used by the ingestion service to build the graph.\n# ==========================================================================\n_LANG_PARSER_REGISTRY = {\n    \"python\":     (\"ingestion_service.languages.python\",     \"PythonLangTreeSitterParser\"),\n    \"javascript\": (\"ingestion_service.languages.javascript\", \"JavascriptLangTreeSitterParser\"),\n    \"typescript\": (\"ingestion_service.languages.typescript\", \"TypescriptLangTreeSitterParser\"),\n    \"go\":         (\"ingestion_service.languages.go\",         \"GoLangTreeSitterParser\"),\n    \"java\":       (\"ingestion_service.languages.java\",       \"JavaLangTreeSitterParser\"),\n    \"rust\":       (\"ingestion_service.languages.rust\",       \"RustLangTreeSitterParser\"),\n    \"c\":          (\"ingestion_service.languages.c\",          \"CLangTreeSitterParser\"),\n    \"cpp\":        (\"ingestion_service.languages.cpp\",        \"CppLangTreeSitterParser\"),\n    \"ruby\":       (\"ingestion_service.languages.ruby\",       \"RubyLangTreeSitterParser\"),\n    \"c_sharp\":    (\"ingestion_service.languages.csharp\",     \"CSharpLangTreeSitterParser\"),\n    \"php\":        (\"ingestion_service.languages.php\",        \"PhpLangTreeSitterParser\"),\n    \"kotlin\":     (\"ingestion_service.languages.kotlin\",     \"KotlinLangTreeSitterParser\"),\n    \"scala\":      (\"ingestion_service.languages.scala\",      \"ScalaLangTreeSitterParser\"),\n    \"swift\":      (\"ingestion_service.languages.swift\",      \"SwiftLangTreeSitterParser\"),\n    \"haskell\":    (\"ingestion_service.languages.haskell\",    \"HaskellLangTreeSitterParser\"),\n}\n\n# Cache so we only instantiate each language parser once\n_parser_cache: Dict[str, object] = {}\n\n\n# ==========================================================================\n# Parser loading\n# ==========================================================================\n\ndef _get_lang_parser(lang: str):\n    \"\"\"\n    Get the tree-sitter language parser for `lang`.\n\n    Each language parser (e.g. PythonLangTreeSitterParser) expects a wrapper\n    with .language_name, .language, and .parser attributes.  We build a\n    lightweight adapter instead of pulling in the full ingestion TreeSitterParser\n    which depends on Neo4j/jobs.\n    \"\"\"\n    # Return from cache if already loaded\n    if lang in _parser_cache:\n        return _parser_cache[lang]\n\n    # Unknown language → skip\n    if lang not in _LANG_PARSER_REGISTRY:\n        return None\n\n    module_path, class_name = _LANG_PARSER_REGISTRY[lang]\n    try:\n        from common.tree_sitter_manager import get_language_safe, create_parser\n\n        # Build the adapter the language parser constructors expect\n        class _Adapter:\n            pass\n        adapter = _Adapter()\n        adapter.language_name = lang\n        adapter.language = get_language_safe(lang)   # tree-sitter Language object\n        adapter.parser = create_parser(lang)         # tree-sitter Parser object\n\n        # Dynamically import the language-specific parser class\n        module = importlib.import_module(module_path)\n        parser_class = getattr(module, class_name)\n\n        # Instantiate and configure\n        parser_instance = parser_class(adapter)\n        parser_instance.index_source = False  # we don't need source code stored on each node\n\n        _parser_cache[lang] = parser_instance\n        return parser_instance\n    except Exception as e:\n        logger.warning(f\"Failed to load parser for {lang}: {e}\")\n        return None\n\n\n# ==========================================================================\n# Diff parsing helpers\n# ==========================================================================\n\ndef _extract_added_source_by_file(diff_text: str) -> Dict[str, str]:\n    \"\"\"\n    Walk through a unified diff and collect added lines per file.\n\n    Returns {\"app/services/foo.py\": \"from x import y\\\\ndef bar(): ...\"}\n\n    Only lines starting with '+' (but not '+++ b/...') are collected,\n    with the leading '+' stripped so tree-sitter can parse the source.\n    \"\"\"\n    files: Dict[str, List[str]] = {}\n    current_file = None\n\n    for line in diff_text.splitlines():\n        # \"+++ b/path/to/file\" marks the start of a new file in the diff\n        if line.startswith(\"+++ b/\"):\n            current_file = line[6:]  # strip \"+++ b/\" prefix\n            if current_file not in files:\n                files[current_file] = []\n        # Lines starting with \"+\" are added lines (skip the \"+++ b/\" header)\n        elif line.startswith(\"+\") and not line.startswith(\"+++\") and current_file:\n            files[current_file].append(line[1:])  # strip the leading \"+\"\n\n    # Join lines into a single source string per file, skip empty files\n    return {f: \"\\n\".join(lines) for f, lines in files.items() if lines}\n\n\n# ==========================================================================\n# Import & symbol extraction from diff (tree-sitter powered)\n# ==========================================================================\n\ndef extract_imports_from_diff(diff_text: str) -> List[Dict]:\n    \"\"\"\n    Extract imports from the added lines of a diff using tree-sitter.\n\n    Uses the same language parsers as the ingestion service, so the import\n    format matches what the graph stores:\n      Python:     {\"name\": \"foo\", \"full_import_name\": \"bar.foo\", ...}\n      JavaScript: {\"name\": \"useState\", \"source\": \"react\", ...}\n    \"\"\"\n    all_imports: List[Dict] = []\n\n    # Group added source by file path\n    added_source = _extract_added_source_by_file(diff_text)\n\n    for file_path, source in added_source.items():\n        # Determine language from file extension\n        ext = Path(file_path).suffix.lower()\n        lang = _EXT_TO_LANG.get(ext)\n        if not lang:\n            continue\n\n        # Load the language-specific parser\n        parser = _get_lang_parser(lang)\n        if not parser or not hasattr(parser, \"_find_imports\"):\n            continue\n\n        try:\n            # Parse the added source into a tree-sitter AST\n            tree = parser.parser.parse(source.encode(\"utf-8\"))\n            # Use the parser's own _find_imports to extract structured import data\n            file_imports = parser._find_imports(tree.root_node)\n            all_imports.extend(file_imports)\n        except Exception as e:\n            logger.warning(f\"Import extraction failed for {file_path}: {e}\")\n\n    return all_imports\n\n\ndef extract_symbols_from_diff(diff_text: str) -> Tuple[Set[str], Set[str]]:\n    \"\"\"\n    Extract function and class names defined in the added lines of a diff.\n\n    Returns (function_names, class_names) as sets of strings.\n    \"\"\"\n    functions: Set[str] = set()\n    classes: Set[str] = set()\n\n    added_source = _extract_added_source_by_file(diff_text)\n\n    print('----- The Added source for the diff -----')\n    print(added_source)\n    print('-----------------------------------------------------')\n\n    for file_path, source in added_source.items():\n        #get the file extension\n        ext = Path(file_path).suffix.lower()\n\n        #get the language from the file extension\n        lang = _EXT_TO_LANG.get(ext)\n        if not lang:\n            continue\n\n        #get the language-specific parser\n        parser = _get_lang_parser(lang)\n        if not parser:\n            continue\n\n        try:\n            #parse the added source into a tree-sitter AST\n            tree = parser.parser.parse(source.encode(\"utf-8\"))\n            root = tree.root_node\n\n            # Extract function definitions\n            if hasattr(parser, \"_find_functions\"):\n                for func in parser._find_functions(root):\n                    functions.add(func.get(\"name\", \"\"))\n\n            # Extract class definitions\n            if hasattr(parser, \"_find_classes\"):\n                for cls in parser._find_classes(root):\n                    classes.add(cls.get(\"name\", \"\"))\n        except Exception as e:\n            logger.warning(f\"Symbol extraction failed for {file_path}: {e}\")\n\n    return functions, classes\n\n\n# ==========================================================================\n# Graph-based resolution\n# ==========================================================================\n\ndef _resolve_imports_from_graph(\n    neo4j_client: Neo4jClient, imports: List[Dict], repo_id: str,\n) -> List[Dict]:\n    \"\"\"\n    For each imported name, search the Neo4j graph for its definition.\n\n    Queries directly with repo filtering (WHERE n.repo = $repo) so results\n    are scoped to the PR's repository. Searches Function, Class, and Variable\n    nodes by exact name match.\n\n    Returns a list of resolved import dicts with source code from the graph.\n    \"\"\"\n    resolved = []\n    seen = set()  # avoid looking up the same name twice\n\n    with neo4j_client.driver.session() as session:\n        for imp in imports:\n            name = imp.get(\"name\", \"\")\n\n            # Skip empty, already-seen, or wildcard imports\n            if not name or name in seen or name == \"*\":\n                continue\n            seen.add(name)\n\n            # full_import_name for Python (\"app.services.foo.bar\"),\n            # source for JS/TS (\"react\", \"./utils\")\n            module = imp.get(\"full_import_name\", imp.get(\"source\", \"\"))\n\n            try:\n                # Search for the symbol in the graph, scoped to this repo\n                result = session.run(\"\"\"\n                    MATCH (n)\n                    WHERE (n:Function OR n:Class OR n:Variable)\n                      AND n.name = $name\n                      AND n.repo = $repo\n                    RETURN n.name AS name, n.path AS path,\n                           n.line_number AS line_number,\n                           n.source AS source, n.docstring AS docstring,\n                           labels(n) AS labels\n                    LIMIT 1\n                \"\"\", name=name, repo=repo_id)\n\n                records = result.data()\n                if not records:\n                    continue  # symbol not in graph (external dep or not ingested)\n\n                r = records[0]\n                labels = r.get(\"labels\", [])\n                node_type = \"class\" if \"Class\" in labels else \"function\"\n\n                resolved.append({\n                    \"name\": name,\n                    \"module\": module,\n                    \"type\": node_type,\n                    \"source\": r.get(\"source\", \"\"),\n                    \"path\": r.get(\"path\", \"\"),\n                    \"line\": r.get(\"line_number\"),\n                    \"docstring\": r.get(\"docstring\", \"\"),\n                })\n            except Exception as e:\n                logger.warning(f\"Failed to resolve import {name}: {e}\")\n\n    return resolved\n\n\ndef _find_callers(code_finder, symbol_names: Set[str]) -> List[Dict]:\n    \"\"\"\n    For each changed symbol, find functions that call it (impact analysis).\n\n    Uses CodeFinder.who_calls_function which queries CALLS relationships\n    in the graph.  Returns at most 10 callers per symbol.\n    \"\"\"\n    callers = []\n\n    for name in symbol_names:\n        try:\n            results = code_finder.who_calls_function(name)\n            if not results:\n                continue\n\n            # Extract caller info, limit to 10 per symbol\n            caller_list = [\n                {\n                    \"name\": c.get(\"caller_function\"),\n                    \"path\": c.get(\"caller_file_path\"),\n                    \"line\": c.get(\"caller_line_number\"),\n                }\n                for c in results\n                if c.get(\"caller_function\")\n            ][:10]\n\n            if caller_list:\n                callers.append({\"symbol\": name, \"callers\": caller_list})\n        except Exception as e:\n            logger.warning(f\"Failed to find callers for {name}: {e}\")\n\n    return callers\n\n\n# ==========================================================================\n# Context formatting (for LLM agents)\n# ==========================================================================\n\ndef _build_agent_context(\n    import_context: List[Dict],\n    caller_context: List[Dict],\n    graph_section: str,\n) -> str:\n    \"\"\"\n    Build the markdown context string that gets sent to the LLM review agents.\n\n    Combines:\n      - Resolved import source code from the graph\n      - Impact analysis (who calls the changed code)\n      - Graph context (symbols in changed line ranges)\n    \"\"\"\n    parts = []\n\n    # Section 1: Source code of imported dependencies\n    if import_context:\n        parts.append(\"## Imported Dependencies (source from graph)\")\n        for imp in import_context:\n            source = imp.get(\"source\", \"\")\n            if source:\n                parts.append(f\"### `{imp['name']}` from `{imp['module']}` ({imp.get('path', '')})\")\n                parts.append(f\"```python\\n{source}\\n```\")\n            else:\n                parts.append(f\"- `{imp['name']}` from `{imp['module']}` (source not in graph)\")\n        parts.append(\"\")\n\n    # Section 2: Callers of changed functions\n    if caller_context:\n        parts.append(\"## Impact Analysis (who calls the changed code)\")\n        for entry in caller_context:\n            caller_names = \", \".join(\n                f\"`{c['name']}` in `{c.get('path', '?')}`\" for c in entry[\"callers\"]\n            )\n            parts.append(f\"- `{entry['symbol']}` is called by: {caller_names}\")\n        parts.append(\"\")\n\n    # Section 3: Graph context (symbols overlapping changed lines)\n    if graph_section and graph_section != \"No graph context available.\":\n        parts.append(\"## Graph Context\")\n        parts.append(graph_section)\n        parts.append(\"\")\n\n    return \"\\n\".join(parts) if parts else \"No additional context available.\"\n\n\n# ==========================================================================\n# Debug dump (writes context to output/<pr_number>/context_<timestamp>.md)\n# ==========================================================================\n\ndef _dump_context_to_file(\n    owner: str, repo: str, pr_number: int, diff_text: str,\n    agent_context: str, raw_imports: List[Dict], resolved_imports: List[Dict],\n    caller_context: List[Dict], graph_symbols: list,\n    files_changed: list, changed_symbol_names: list, risk_level: str,\n) -> None:\n    \"\"\"Save all gathered context to a markdown file for debugging/inspection.\"\"\"\n    try:\n        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n        output_dir = Path(\"output\") / str(pr_number)\n        output_dir.mkdir(parents=True, exist_ok=True)\n        context_file = output_dir / f\"context_{timestamp}.md\"\n\n        total_callers = sum(len(c[\"callers\"]) for c in caller_context)\n\n        lines = [\n            f\"# PR Review Context — {owner}/{repo}#{pr_number}\",\n            f\"**Generated at:** {datetime.now().isoformat()}\",\n            f\"**Risk level:** {risk_level}\",\n            \"\",\n            # What files were touched\n            \"## Files Changed\",\n            *[f\"- `{f}`\" for f in files_changed],\n            \"\",\n            # Functions/classes defined in the added lines\n            \"## Changed Symbols (from diff)\",\n            *[f\"- `{s}`\" for s in changed_symbol_names],\n            \"\",\n            # All imports found in the added lines (tree-sitter extraction)\n            f\"## Extracted Imports from Diff ({len(raw_imports)})\",\n            *[f\"- `{imp.get('name', '?')}` (full: `{imp.get('full_import_name', imp.get('source', '?'))}`)\"\n              for imp in raw_imports],\n            \"\",\n            # Which of those imports we found in the graph (with source code)\n            f\"## Resolved Imports ({len(resolved_imports)}/{len(raw_imports)} resolved from graph)\",\n            *[f\"- `{imp['name']}` from `{imp['module']}` → `{imp.get('path', 'not found')}`\"\n              for imp in resolved_imports],\n            \"\",\n            # Functions that call the changed symbols\n            f\"## Caller Impact ({total_callers} callers)\",\n            *[f\"- `{c['symbol']}` called by: \"\n              + \", \".join(f\"`{caller['name']}`\" for caller in c[\"callers\"])\n              for c in caller_context],\n            \"\",\n            # Symbols from the graph that overlap with changed line ranges\n            f\"## Graph Symbols ({len(graph_symbols)})\",\n            *[f\"- `{s.get('name', '')}` ({s.get('type', 'unknown')}) in `{s.get('file_path', '')}`\"\n              for s in graph_symbols],\n            \"\",\n            # The full context string that gets sent to the LLM agents\n            \"## Context Sent to Agents\",\n            agent_context,\n            \"\",\n            # Raw diff for reference\n            \"## Diff\",\n            \"```diff\",\n            diff_text,\n            \"```\",\n        ]\n\n        context_file.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n        logger.info(f\"Review context saved to {context_file}\")\n    except Exception as e:\n        logger.warning(f\"Failed to dump review context: {e}\")\n\n\n# ==========================================================================\n# Main pipeline\n# ==========================================================================\n\nasync def execute_pr_review(owner: str, repo: str, pr_number: int) -> None:\n    \"\"\"\n    Full PR review pipeline.\n\n    Steps:\n      1. Fetch the PR diff from GitHub\n      2. Parse the diff into changed file paths and line ranges\n      3. Extract imports and symbol definitions from added lines (tree-sitter)\n      4. Connect to Neo4j and resolve context:\n         a. Find graph symbols overlapping changed line ranges\n         b. Look up imported symbols in the graph (get source code)\n         c. Find callers of changed functions (impact analysis)\n      5. Build risk level based on scope of changes\n      6. Format everything into context for the LLM agents\n      7. Dump context to file for debugging\n      8. Run multi-agent review (bug-hunter + security-auditor)\n      9. Format and post the review comment on the PR\n    \"\"\"\n    try:\n        logger.info(f\"Starting review pipeline for {owner}/{repo}#{pr_number}\")\n        repo_id = f\"{owner}/{repo}\"  # matches the repo identifier stored in the graph\n\n        # ── Step 1: Fetch diff from GitHub ──────────────────────────────\n        gh = GitHubClient()\n        diff_text = await gh.get_pr_diff(owner, repo, pr_number)\n        if not diff_text:\n            logger.warning(\"Empty diff, skipping review\")\n            return\n        logger.info(f\"Fetched diff ({len(diff_text)} chars)\")\n\n        # ── Step 2: Parse diff structure ────────────────────────────────\n        # Returns list of {\"file_path\", \"start_line\", \"end_line\"} hunks\n        changes = parse_unified_diff(diff_text)\n        files_changed = list({c[\"file_path\"] for c in changes})\n        logger.info(f\"Parsed {len(changes)} hunks across {len(files_changed)} files\")\n\n        # ── Step 3: Extract imports & symbols from added lines ──────────\n        # Uses the same tree-sitter language parsers as the ingestion service\n        diff_imports = extract_imports_from_diff(diff_text)\n\n        print('-----------------------------------------------------')\n        print(diff_imports)\n        print('-----------------------------------------------------')\n\n        diff_functions, diff_classes = extract_symbols_from_diff(diff_text)\n\n        print('-----------------------------------------------------')\n        print(diff_functions)\n        print(diff_classes)\n        print('-----------------------------------------------------')\n\n    \n        all_diff_symbols = diff_functions | diff_classes\n        logger.info(\n            f\"Diff extraction: {len(diff_imports)} imports, \"\n            f\"{len(diff_functions)} functions, {len(diff_classes)} classes\"\n        )\n\n        # ── Step 4: Connect to Neo4j and resolve context ────────────────\n        neo4j = Neo4jClient(\n            uri=os.environ.get(\"NEO4J_URI\", \"\"),\n            user=os.environ.get(\"NEO4J_USERNAME\", \"neo4j\"),\n            password=os.environ.get(\"NEO4J_PASSWORD\", \"\"),\n            database=os.environ.get(\"NEO4J_DATABASE\", \"neo4j\"),\n        )\n        query_service = CodeQueryService(neo4j)\n        from api.services.code_search import CodeFinder\n        code_finder = CodeFinder(neo4j)\n\n        # 4a. Graph symbols overlapping the changed line ranges\n        graph_context = query_service.get_diff_context_enhanced(repo_id, changes)\n        graph_symbols = graph_context.get(\"affected_symbols\", [])\n        graph_section = build_graph_context_section(graph_context)\n\n        # 4b. Look up each imported name in the graph (scoped to this repo)\n        resolved_imports = _resolve_imports_from_graph(neo4j, diff_imports, repo_id)\n        logger.info(f\"Resolved {len(resolved_imports)}/{len(diff_imports)} imports from graph\")\n\n        # 4c. Find callers of changed functions (impact analysis)\n        # Merge symbols from diff extraction + graph overlap\n        all_symbol_names = all_diff_symbols | {s.get(\"name\", \"\") for s in graph_symbols}\n        caller_context = _find_callers(code_finder, all_symbol_names)\n        total_callers = sum(len(c[\"callers\"]) for c in caller_context)\n        logger.info(f\"Found {total_callers} callers across {len(caller_context)} symbols\")\n\n        # ── Step 5: Determine risk level ────────────────────────────────\n        changed_symbol_names = list(all_symbol_names)\n        if total_callers > 10 or len(all_symbol_names) > 5:\n            risk_level = \"high\"\n        elif total_callers > 3 or len(all_symbol_names) > 2:\n            risk_level = \"medium\"\n        else:\n            risk_level = \"low\"\n\n        # ── Step 6: Build context for LLM agents ───────────────────────\n        agent_context = _build_agent_context(\n            resolved_imports, caller_context, graph_section\n        )\n\n        # ── Step 7: Dump context to file for debugging ──────────────────\n        _dump_context_to_file(\n            owner, repo, pr_number, diff_text,\n            agent_context, diff_imports, resolved_imports, caller_context,\n            graph_symbols, files_changed, changed_symbol_names, risk_level,\n        )\n\n        # ── Step 8: Run multi-agent review ──────────────────────────────\n        review_results = await run_review(diff_text, agent_context, repo_id, pr_number)\n        logger.info(f\"Review complete: {len(review_results.issues)} issues found\")\n\n        # ── Step 9: Format and post comment ─────────────────────────────\n        context_data = ContextData(\n            files_changed=files_changed,\n            modified_symbols=changed_symbol_names,\n            total_callers=total_callers,\n            risk_level=risk_level,\n        )\n        final_comment = format_github_comment(review_results, context_data, pr_number)\n        await gh.post_comment(owner, repo, pr_number, final_comment)\n        logger.info(f\"Posted review comment on {owner}/{repo}#{pr_number}\")\n\n    except Exception:\n        logger.error(f\"Review pipeline failed: {traceback.format_exc()}\")\n","structuredPatch":[{"oldStart":401,"oldLines":73,"newStart":401,"newLines":24,"lines":[" "," "," # ==========================================================================","-# Debug dump (writes context to output/<pr_number>/context_<timestamp>.md)","+# Per-step debug dump helpers"," # =========================================================================="," ","-def _dump_context_to_file(","-    owner: str, repo: str, pr_number: int, diff_text: str,","-    agent_context: str, raw_imports: List[Dict], resolved_imports: List[Dict],","-    caller_context: List[Dict], graph_symbols: list,","-    files_changed: list, changed_symbol_names: list, risk_level: str,","-) -> None:","-    \"\"\"Save all gathered context to a markdown file for debugging/inspection.\"\"\"","-    try:","-        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")","-        output_dir = Path(\"output\") / str(pr_number)","-        output_dir.mkdir(parents=True, exist_ok=True)","-        context_file = output_dir / f\"context_{timestamp}.md\"","+def _make_review_dir(owner: str, repo: str, pr_number: int) -> Path:","+    \"\"\"Create and return a timestamped folder for this review run.\"\"\"","+    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")","+    review_dir = Path(\"output\") / f\"review-{timestamp}\"","+    review_dir.mkdir(parents=True, exist_ok=True)","+    logger.info(f\"Review debug dir: {review_dir}\")","+    return review_dir"," ","-        total_callers = sum(len(c[\"callers\"]) for c in caller_context)"," ","-        lines = [","-            f\"# PR Review Context — {owner}/{repo}#{pr_number}\",","-            f\"**Generated at:** {datetime.now().isoformat()}\",","-            f\"**Risk level:** {risk_level}\",","-            \"\",","-            # What files were touched","-            \"## Files Changed\",","-            *[f\"- `{f}`\" for f in files_changed],","-            \"\",","-            # Functions/classes defined in the added lines","-            \"## Changed Symbols (from diff)\",","-            *[f\"- `{s}`\" for s in changed_symbol_names],","-            \"\",","-            # All imports found in the added lines (tree-sitter extraction)","-            f\"## Extracted Imports from Diff ({len(raw_imports)})\",","-            *[f\"- `{imp.get('name', '?')}` (full: `{imp.get('full_import_name', imp.get('source', '?'))}`)\"","-              for imp in raw_imports],","-            \"\",","-            # Which of those imports we found in the graph (with source code)","-            f\"## Resolved Imports ({len(resolved_imports)}/{len(raw_imports)} resolved from graph)\",","-            *[f\"- `{imp['name']}` from `{imp['module']}` → `{imp.get('path', 'not found')}`\"","-              for imp in resolved_imports],","-            \"\",","-            # Functions that call the changed symbols","-            f\"## Caller Impact ({total_callers} callers)\",","-            *[f\"- `{c['symbol']}` called by: \"","-              + \", \".join(f\"`{caller['name']}`\" for caller in c[\"callers\"])","-              for c in caller_context],","-            \"\",","-            # Symbols from the graph that overlap with changed line ranges","-            f\"## Graph Symbols ({len(graph_symbols)})\",","-            *[f\"- `{s.get('name', '')}` ({s.get('type', 'unknown')}) in `{s.get('file_path', '')}`\"","-              for s in graph_symbols],","-            \"\",","-            # The full context string that gets sent to the LLM agents","-            \"## Context Sent to Agents\",","-            agent_context,","-            \"\",","-            # Raw diff for reference","-            \"## Diff\",","-            \"```diff\",","-            diff_text,","-            \"```\",","-        ]","-","-        context_file.write_text(\"\\n\".join(lines), encoding=\"utf-8\")","-        logger.info(f\"Review context saved to {context_file}\")","+def _write_step(review_dir: Path, filename: str, content: str) -> None:","+    \"\"\"Write a single step's debug output to <review_dir>/<filename>.md.\"\"\"","+    try:","+        (review_dir / filename).write_text(content, encoding=\"utf-8\")","     except Exception as e:","-        logger.warning(f\"Failed to dump review context: {e}\")","+        logger.warning(f\"Failed to write {filename}: {e}\")"," "," "," # =========================================================================="]}],"userModified":false,"replaceAll":false},"sourceToolAssistantUUID":"dcb9782c-6280-46c9-828d-e4465b3cd525"}
{"parentUuid":"4d539561-8b00-432a-bdaf-244c4c9998da","isSidechain":false,"userType":"external","cwd":"/Users/skmabudalam/Documents/BugViper","sessionId":"0f25fa6f-5425-4cfc-8c2e-2b3c7eda02c7","version":"2.1.39","gitBranch":"review-pipeline-improvements","slug":"immutable-launching-music","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_014GoabgKAJqkjY5zRGmbTZF","type":"message","role":"assistant","content":[{"type":"text","text":"Now replace the `execute_pr_review` body:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1890,"cache_read_input_tokens":32324,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1890},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY5g81MRAHCvxYwrUwzpR","type":"assistant","uuid":"bf902518-42ea-41aa-a11f-a5bd5a8f6795","timestamp":"2026-02-13T07:29:37.648Z"}
{"parentUuid":"bf902518-42ea-41aa-a11f-a5bd5a8f6795","isSidechain":false,"userType":"external","cwd":"/Users/skmabudalam/Documents/BugViper","sessionId":"0f25fa6f-5425-4cfc-8c2e-2b3c7eda02c7","version":"2.1.39","gitBranch":"review-pipeline-improvements","slug":"immutable-launching-music","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_014GoabgKAJqkjY5zRGmbTZF","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01GBKpYGdaMiv5N7YCbsWUsf","name":"Edit","input":{"replace_all":false,"file_path":"/Users/skmabudalam/Documents/BugViper/api/services/review_service.py","old_string":"async def execute_pr_review(owner: str, repo: str, pr_number: int) -> None:\n    \"\"\"\n    Full PR review pipeline.\n\n    Steps:\n      1. Fetch the PR diff from GitHub\n      2. Parse the diff into changed file paths and line ranges\n      3. Extract imports and symbol definitions from added lines (tree-sitter)\n      4. Connect to Neo4j and resolve context:\n         a. Find graph symbols overlapping changed line ranges\n         b. Look up imported symbols in the graph (get source code)\n         c. Find callers of changed functions (impact analysis)\n      5. Build risk level based on scope of changes\n      6. Format everything into context for the LLM agents\n      7. Dump context to file for debugging\n      8. Run multi-agent review (bug-hunter + security-auditor)\n      9. Format and post the review comment on the PR\n    \"\"\"\n    try:\n        logger.info(f\"Starting review pipeline for {owner}/{repo}#{pr_number}\")\n        repo_id = f\"{owner}/{repo}\"  # matches the repo identifier stored in the graph\n\n        # ── Step 1: Fetch diff from GitHub ──────────────────────────────\n        gh = GitHubClient()\n        diff_text = await gh.get_pr_diff(owner, repo, pr_number)\n        if not diff_text:\n            logger.warning(\"Empty diff, skipping review\")\n            return\n        logger.info(f\"Fetched diff ({len(diff_text)} chars)\")\n\n        # ── Step 2: Parse diff structure ────────────────────────────────\n        # Returns list of {\"file_path\", \"start_line\", \"end_line\"} hunks\n        changes = parse_unified_diff(diff_text)\n        files_changed = list({c[\"file_path\"] for c in changes})\n        logger.info(f\"Parsed {len(changes)} hunks across {len(files_changed)} files\")\n\n        # ── Step 3: Extract imports & symbols from added lines ──────────\n        # Uses the same tree-sitter language parsers as the ingestion service\n        diff_imports = extract_imports_from_diff(diff_text)\n\n        print('-----------------------------------------------------')\n        print(diff_imports)\n        print('-----------------------------------------------------')\n\n        diff_functions, diff_classes = extract_symbols_from_diff(diff_text)\n\n        print('-----------------------------------------------------')\n        print(diff_functions)\n        print(diff_classes)\n        print('-----------------------------------------------------')\n\n    \n        all_diff_symbols = diff_functions | diff_classes\n        logger.info(\n            f\"Diff extraction: {len(diff_imports)} imports, \"\n            f\"{len(diff_functions)} functions, {len(diff_classes)} classes\"\n        )\n\n        # ── Step 4: Connect to Neo4j and resolve context ────────────────\n        neo4j = Neo4jClient(\n            uri=os.environ.get(\"NEO4J_URI\", \"\"),\n            user=os.environ.get(\"NEO4J_USERNAME\", \"neo4j\"),\n            password=os.environ.get(\"NEO4J_PASSWORD\", \"\"),\n            database=os.environ.get(\"NEO4J_DATABASE\", \"neo4j\"),\n        )\n        query_service = CodeQueryService(neo4j)\n        from api.services.code_search import CodeFinder\n        code_finder = CodeFinder(neo4j)\n\n        # 4a. Graph symbols overlapping the changed line ranges\n        graph_context = query_service.get_diff_context_enhanced(repo_id, changes)\n        graph_symbols = graph_context.get(\"affected_symbols\", [])\n        graph_section = build_graph_context_section(graph_context)\n\n        # 4b. Look up each imported name in the graph (scoped to this repo)\n        resolved_imports = _resolve_imports_from_graph(neo4j, diff_imports, repo_id)\n        logger.info(f\"Resolved {len(resolved_imports)}/{len(diff_imports)} imports from graph\")\n\n        # 4c. Find callers of changed functions (impact analysis)\n        # Merge symbols from diff extraction + graph overlap\n        all_symbol_names = all_diff_symbols | {s.get(\"name\", \"\") for s in graph_symbols}\n        caller_context = _find_callers(code_finder, all_symbol_names)\n        total_callers = sum(len(c[\"callers\"]) for c in caller_context)\n        logger.info(f\"Found {total_callers} callers across {len(caller_context)} symbols\")\n\n        # ── Step 5: Determine risk level ────────────────────────────────\n        changed_symbol_names = list(all_symbol_names)\n        if total_callers > 10 or len(all_symbol_names) > 5:\n            risk_level = \"high\"\n        elif total_callers > 3 or len(all_symbol_names) > 2:\n            risk_level = \"medium\"\n        else:\n            risk_level = \"low\"\n\n        # ── Step 6: Build context for LLM agents ───────────────────────\n        agent_context = _build_agent_context(\n            resolved_imports, caller_context, graph_section\n        )\n\n        # ── Step 7: Dump context to file for debugging ──────────────────\n        _dump_context_to_file(\n            owner, repo, pr_number, diff_text,\n            agent_context, diff_imports, resolved_imports, caller_context,\n            graph_symbols, files_changed, changed_symbol_names, risk_level,\n        )\n\n        # ── Step 8: Run multi-agent review ──────────────────────────────\n        review_results = await run_review(diff_text, agent_context, repo_id, pr_number)\n        logger.info(f\"Review complete: {len(review_results.issues)} issues found\")\n\n        # ── Step 9: Format and post comment ─────────────────────────────\n        context_data = ContextData(\n            files_changed=files_changed,\n            modified_symbols=changed_symbol_names,\n            total_callers=total_callers,\n            risk_level=risk_level,\n        )\n        final_comment = format_github_comment(review_results, context_data, pr_number)\n        await gh.post_comment(owner, repo, pr_number, final_comment)\n        logger.info(f\"Posted review comment on {owner}/{repo}#{pr_number}\")\n\n    except Exception:\n        logger.error(f\"Review pipeline failed: {traceback.format_exc()}\")","new_string":"async def execute_pr_review(owner: str, repo: str, pr_number: int) -> None:\n    \"\"\"\n    Full PR review pipeline.\n\n    Steps:\n      1. Fetch the PR diff from GitHub\n      2. Parse the diff into changed file paths and line ranges\n      3. Extract imports and symbol definitions from added lines (tree-sitter)\n      4. Connect to Neo4j and resolve context:\n         a. Find graph symbols overlapping changed line ranges\n         b. Look up imported symbols in the graph (get source code)\n         c. Find callers of changed functions (impact analysis)\n      5. Build risk level based on scope of changes\n      6. Format everything into context for the LLM agents\n      7. [DEBUG MODE] LLM call skipped — all context written to review dir\n    \"\"\"\n    try:\n        logger.info(f\"Starting review pipeline for {owner}/{repo}#{pr_number}\")\n        repo_id = f\"{owner}/{repo}\"  # matches the repo identifier stored in the graph\n\n        # Create a timestamped folder for this review run\n        review_dir = _make_review_dir(owner, repo, pr_number)\n\n        # ── Step 1: Fetch diff from GitHub ──────────────────────────────\n        gh = GitHubClient()\n        diff_text = await gh.get_pr_diff(owner, repo, pr_number)\n        if not diff_text:\n            logger.warning(\"Empty diff, skipping review\")\n            return\n        logger.info(f\"Fetched diff ({len(diff_text)} chars)\")\n\n        _write_step(review_dir, \"01_diff.md\", \"\\n\".join([\n            f\"# Step 1 — Raw Diff\",\n            f\"**PR:** {owner}/{repo}#{pr_number}\",\n            f\"**Chars:** {len(diff_text)}\",\n            \"\",\n            \"```diff\",\n            diff_text,\n            \"```\",\n        ]))\n\n        # ── Step 2: Parse diff structure ────────────────────────────────\n        changes = parse_unified_diff(diff_text)\n        files_changed = list({c[\"file_path\"] for c in changes})\n        logger.info(f\"Parsed {len(changes)} hunks across {len(files_changed)} files\")\n\n        _write_step(review_dir, \"02_parsed_diff.md\", \"\\n\".join([\n            f\"# Step 2 — Parsed Diff\",\n            f\"**Hunks:** {len(changes)}  |  **Files changed:** {len(files_changed)}\",\n            \"\",\n            \"## Files Changed\",\n            *[f\"- `{f}`\" for f in files_changed],\n            \"\",\n            \"## Hunks\",\n            *[\n                f\"- `{c['file_path']}` lines {c.get('start_line', '?')}–{c.get('end_line', '?')}\"\n                for c in changes\n            ],\n        ]))\n\n        # ── Step 3: Extract imports & symbols from added lines ──────────\n        diff_imports = extract_imports_from_diff(diff_text)\n        diff_functions, diff_classes = extract_symbols_from_diff(diff_text)\n        all_diff_symbols = diff_functions | diff_classes\n        logger.info(\n            f\"Diff extraction: {len(diff_imports)} imports, \"\n            f\"{len(diff_functions)} functions, {len(diff_classes)} classes\"\n        )\n\n        _write_step(review_dir, \"03_extracted_symbols.md\", \"\\n\".join([\n            f\"# Step 3 — Extracted Imports & Symbols\",\n            \"\",\n            f\"## Imports ({len(diff_imports)})\",\n            *[\n                f\"- `{imp.get('name', '?')}` \"\n                f\"(full: `{imp.get('full_import_name', imp.get('source', '?'))}`)\"\n                for imp in diff_imports\n            ],\n            \"\",\n            f\"## Functions ({len(diff_functions)})\",\n            *[f\"- `{fn}`\" for fn in sorted(diff_functions)],\n            \"\",\n            f\"## Classes ({len(diff_classes)})\",\n            *[f\"- `{cls}`\" for cls in sorted(diff_classes)],\n        ]))\n\n        # ── Step 4: Connect to Neo4j and resolve context ────────────────\n        neo4j = Neo4jClient(\n            uri=os.environ.get(\"NEO4J_URI\", \"\"),\n            user=os.environ.get(\"NEO4J_USERNAME\", \"neo4j\"),\n            password=os.environ.get(\"NEO4J_PASSWORD\", \"\"),\n            database=os.environ.get(\"NEO4J_DATABASE\", \"neo4j\"),\n        )\n        query_service = CodeQueryService(neo4j)\n        from api.services.code_search import CodeFinder\n        code_finder = CodeFinder(neo4j)\n\n        # 4a. Graph symbols overlapping the changed line ranges\n        graph_context = query_service.get_diff_context_enhanced(repo_id, changes)\n        graph_symbols = graph_context.get(\"affected_symbols\", [])\n        graph_section = build_graph_context_section(graph_context)\n\n        _write_step(review_dir, \"04a_graph_symbols.md\", \"\\n\".join([\n            f\"# Step 4a — Graph Symbols (overlapping changed lines)\",\n            \"\",\n            f\"## Affected Symbols ({len(graph_symbols)})\",\n            *[\n                f\"- `{s.get('name', '')}` ({s.get('type', 'unknown')}) \"\n                f\"in `{s.get('file_path', '')}`\"\n                for s in graph_symbols\n            ],\n            \"\",\n            \"## Graph Context Section (for agents)\",\n            graph_section,\n        ]))\n\n        # 4b. Look up each imported name in the graph (scoped to this repo)\n        resolved_imports = _resolve_imports_from_graph(neo4j, diff_imports, repo_id)\n        logger.info(f\"Resolved {len(resolved_imports)}/{len(diff_imports)} imports from graph\")\n\n        _write_step(review_dir, \"04b_resolved_imports.md\", \"\\n\".join([\n            f\"# Step 4b — Resolved Imports from Graph\",\n            f\"**Resolved:** {len(resolved_imports)} / {len(diff_imports)}\",\n            \"\",\n            *[\n                \"\\n\".join([\n                    f\"## `{imp['name']}` from `{imp['module']}`\",\n                    f\"- **Path:** `{imp.get('path', 'n/a')}`\",\n                    f\"- **Type:** {imp.get('type', 'n/a')}\",\n                    f\"- **Line:** {imp.get('line', 'n/a')}\",\n                    f\"- **Docstring:** {imp.get('docstring', '') or '_none_'}\",\n                    \"\",\n                    \"```python\",\n                    imp.get(\"source\", \"\") or \"# source not available\",\n                    \"```\",\n                ])\n                for imp in resolved_imports\n            ],\n        ]))\n\n        # 4c. Find callers of changed functions (impact analysis)\n        all_symbol_names = all_diff_symbols | {s.get(\"name\", \"\") for s in graph_symbols}\n        caller_context = _find_callers(code_finder, all_symbol_names)\n        total_callers = sum(len(c[\"callers\"]) for c in caller_context)\n        logger.info(f\"Found {total_callers} callers across {len(caller_context)} symbols\")\n\n        _write_step(review_dir, \"04c_caller_context.md\", \"\\n\".join([\n            f\"# Step 4c — Caller Impact Analysis\",\n            f\"**Total callers:** {total_callers}  |  **Symbols with callers:** {len(caller_context)}\",\n            \"\",\n            *[\n                \"\\n\".join([\n                    f\"## `{entry['symbol']}`\",\n                    *[\n                        f\"- `{c['name']}` in `{c.get('path', '?')}` (line {c.get('line', '?')})\"\n                        for c in entry[\"callers\"]\n                    ],\n                ])\n                for entry in caller_context\n            ],\n        ]))\n\n        # ── Step 5: Determine risk level ────────────────────────────────\n        changed_symbol_names = list(all_symbol_names)\n        if total_callers > 10 or len(all_symbol_names) > 5:\n            risk_level = \"high\"\n        elif total_callers > 3 or len(all_symbol_names) > 2:\n            risk_level = \"medium\"\n        else:\n            risk_level = \"low\"\n\n        _write_step(review_dir, \"05_risk_level.md\", \"\\n\".join([\n            f\"# Step 5 — Risk Level\",\n            \"\",\n            f\"**Risk level:** `{risk_level}`\",\n            f\"**Total callers:** {total_callers}\",\n            f\"**Total symbols:** {len(all_symbol_names)}\",\n            \"\",\n            \"## All Changed Symbols\",\n            *[f\"- `{s}`\" for s in sorted(changed_symbol_names)],\n        ]))\n\n        # ── Step 6: Build context for LLM agents ───────────────────────\n        agent_context = _build_agent_context(\n            resolved_imports, caller_context, graph_section\n        )\n\n        _write_step(review_dir, \"06_agent_context.md\", \"\\n\".join([\n            f\"# Step 6 — Agent Context (what would be sent to the LLM)\",\n            \"\",\n            agent_context,\n        ]))\n\n        # ── Step 7: LLM call skipped (debug mode) ──────────────────────\n        logger.info(\n            f\"[DEBUG MODE] LLM call skipped. \"\n            f\"All context written to {review_dir}\"\n        )\n\n    except Exception:\n        logger.error(f\"Review pipeline failed: {traceback.format_exc()}\")"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1890,"cache_read_input_tokens":32324,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1890},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY5g81MRAHCvxYwrUwzpR","type":"assistant","uuid":"54716037-b591-4a4b-aa14-21777d955898","timestamp":"2026-02-13T07:30:20.646Z"}
{"parentUuid":"54716037-b591-4a4b-aa14-21777d955898","isSidechain":false,"userType":"external","cwd":"/Users/skmabudalam/Documents/BugViper","sessionId":"0f25fa6f-5425-4cfc-8c2e-2b3c7eda02c7","version":"2.1.39","gitBranch":"review-pipeline-improvements","slug":"immutable-launching-music","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Edit","command":"callback"},"parentToolUseID":"toolu_01GBKpYGdaMiv5N7YCbsWUsf","toolUseID":"toolu_01GBKpYGdaMiv5N7YCbsWUsf","timestamp":"2026-02-13T07:32:55.598Z","uuid":"8ea9c487-029d-4d53-965b-2e755f4f8208"}
{"parentUuid":"54716037-b591-4a4b-aa14-21777d955898","isSidechain":false,"userType":"external","cwd":"/Users/skmabudalam/Documents/BugViper","sessionId":"0f25fa6f-5425-4cfc-8c2e-2b3c7eda02c7","version":"2.1.39","gitBranch":"review-pipeline-improvements","slug":"immutable-launching-music","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01GBKpYGdaMiv5N7YCbsWUsf","type":"tool_result","content":"The file /Users/skmabudalam/Documents/BugViper/api/services/review_service.py has been updated successfully."}]},"uuid":"08b4b784-c2ef-49eb-be86-498f0d974273","timestamp":"2026-02-13T07:32:55.598Z","toolUseResult":{"filePath":"/Users/skmabudalam/Documents/BugViper/api/services/review_service.py","oldString":"async def execute_pr_review(owner: str, repo: str, pr_number: int) -> None:\n    \"\"\"\n    Full PR review pipeline.\n\n    Steps:\n      1. Fetch the PR diff from GitHub\n      2. Parse the diff into changed file paths and line ranges\n      3. Extract imports and symbol definitions from added lines (tree-sitter)\n      4. Connect to Neo4j and resolve context:\n         a. Find graph symbols overlapping changed line ranges\n         b. Look up imported symbols in the graph (get source code)\n         c. Find callers of changed functions (impact analysis)\n      5. Build risk level based on scope of changes\n      6. Format everything into context for the LLM agents\n      7. Dump context to file for debugging\n      8. Run multi-agent review (bug-hunter + security-auditor)\n      9. Format and post the review comment on the PR\n    \"\"\"\n    try:\n        logger.info(f\"Starting review pipeline for {owner}/{repo}#{pr_number}\")\n        repo_id = f\"{owner}/{repo}\"  # matches the repo identifier stored in the graph\n\n        # ── Step 1: Fetch diff from GitHub ──────────────────────────────\n        gh = GitHubClient()\n        diff_text = await gh.get_pr_diff(owner, repo, pr_number)\n        if not diff_text:\n            logger.warning(\"Empty diff, skipping review\")\n            return\n        logger.info(f\"Fetched diff ({len(diff_text)} chars)\")\n\n        # ── Step 2: Parse diff structure ────────────────────────────────\n        # Returns list of {\"file_path\", \"start_line\", \"end_line\"} hunks\n        changes = parse_unified_diff(diff_text)\n        files_changed = list({c[\"file_path\"] for c in changes})\n        logger.info(f\"Parsed {len(changes)} hunks across {len(files_changed)} files\")\n\n        # ── Step 3: Extract imports & symbols from added lines ──────────\n        # Uses the same tree-sitter language parsers as the ingestion service\n        diff_imports = extract_imports_from_diff(diff_text)\n\n        print('-----------------------------------------------------')\n        print(diff_imports)\n        print('-----------------------------------------------------')\n\n        diff_functions, diff_classes = extract_symbols_from_diff(diff_text)\n\n        print('-----------------------------------------------------')\n        print(diff_functions)\n        print(diff_classes)\n        print('-----------------------------------------------------')\n\n    \n        all_diff_symbols = diff_functions | diff_classes\n        logger.info(\n            f\"Diff extraction: {len(diff_imports)} imports, \"\n            f\"{len(diff_functions)} functions, {len(diff_classes)} classes\"\n        )\n\n        # ── Step 4: Connect to Neo4j and resolve context ────────────────\n        neo4j = Neo4jClient(\n            uri=os.environ.get(\"NEO4J_URI\", \"\"),\n            user=os.environ.get(\"NEO4J_USERNAME\", \"neo4j\"),\n            password=os.environ.get(\"NEO4J_PASSWORD\", \"\"),\n            database=os.environ.get(\"NEO4J_DATABASE\", \"neo4j\"),\n        )\n        query_service = CodeQueryService(neo4j)\n        from api.services.code_search import CodeFinder\n        code_finder = CodeFinder(neo4j)\n\n        # 4a. Graph symbols overlapping the changed line ranges\n        graph_context = query_service.get_diff_context_enhanced(repo_id, changes)\n        graph_symbols = graph_context.get(\"affected_symbols\", [])\n        graph_section = build_graph_context_section(graph_context)\n\n        # 4b. Look up each imported name in the graph (scoped to this repo)\n        resolved_imports = _resolve_imports_from_graph(neo4j, diff_imports, repo_id)\n        logger.info(f\"Resolved {len(resolved_imports)}/{len(diff_imports)} imports from graph\")\n\n        # 4c. Find callers of changed functions (impact analysis)\n        # Merge symbols from diff extraction + graph overlap\n        all_symbol_names = all_diff_symbols | {s.get(\"name\", \"\") for s in graph_symbols}\n        caller_context = _find_callers(code_finder, all_symbol_names)\n        total_callers = sum(len(c[\"callers\"]) for c in caller_context)\n        logger.info(f\"Found {total_callers} callers across {len(caller_context)} symbols\")\n\n        # ── Step 5: Determine risk level ────────────────────────────────\n        changed_symbol_names = list(all_symbol_names)\n        if total_callers > 10 or len(all_symbol_names) > 5:\n            risk_level = \"high\"\n        elif total_callers > 3 or len(all_symbol_names) > 2:\n            risk_level = \"medium\"\n        else:\n            risk_level = \"low\"\n\n        # ── Step 6: Build context for LLM agents ───────────────────────\n        agent_context = _build_agent_context(\n            resolved_imports, caller_context, graph_section\n        )\n\n        # ── Step 7: Dump context to file for debugging ──────────────────\n        _dump_context_to_file(\n            owner, repo, pr_number, diff_text,\n            agent_context, diff_imports, resolved_imports, caller_context,\n            graph_symbols, files_changed, changed_symbol_names, risk_level,\n        )\n\n        # ── Step 8: Run multi-agent review ──────────────────────────────\n        review_results = await run_review(diff_text, agent_context, repo_id, pr_number)\n        logger.info(f\"Review complete: {len(review_results.issues)} issues found\")\n\n        # ── Step 9: Format and post comment ─────────────────────────────\n        context_data = ContextData(\n            files_changed=files_changed,\n            modified_symbols=changed_symbol_names,\n            total_callers=total_callers,\n            risk_level=risk_level,\n        )\n        final_comment = format_github_comment(review_results, context_data, pr_number)\n        await gh.post_comment(owner, repo, pr_number, final_comment)\n        logger.info(f\"Posted review comment on {owner}/{repo}#{pr_number}\")\n\n    except Exception:\n        logger.error(f\"Review pipeline failed: {traceback.format_exc()}\")","newString":"async def execute_pr_review(owner: str, repo: str, pr_number: int) -> None:\n    \"\"\"\n    Full PR review pipeline.\n\n    Steps:\n      1. Fetch the PR diff from GitHub\n      2. Parse the diff into changed file paths and line ranges\n      3. Extract imports and symbol definitions from added lines (tree-sitter)\n      4. Connect to Neo4j and resolve context:\n         a. Find graph symbols overlapping changed line ranges\n         b. Look up imported symbols in the graph (get source code)\n         c. Find callers of changed functions (impact analysis)\n      5. Build risk level based on scope of changes\n      6. Format everything into context for the LLM agents\n      7. [DEBUG MODE] LLM call skipped — all context written to review dir\n    \"\"\"\n    try:\n        logger.info(f\"Starting review pipeline for {owner}/{repo}#{pr_number}\")\n        repo_id = f\"{owner}/{repo}\"  # matches the repo identifier stored in the graph\n\n        # Create a timestamped folder for this review run\n        review_dir = _make_review_dir(owner, repo, pr_number)\n\n        # ── Step 1: Fetch diff from GitHub ──────────────────────────────\n        gh = GitHubClient()\n        diff_text = await gh.get_pr_diff(owner, repo, pr_number)\n        if not diff_text:\n            logger.warning(\"Empty diff, skipping review\")\n            return\n        logger.info(f\"Fetched diff ({len(diff_text)} chars)\")\n\n        _write_step(review_dir, \"01_diff.md\", \"\\n\".join([\n            f\"# Step 1 — Raw Diff\",\n            f\"**PR:** {owner}/{repo}#{pr_number}\",\n            f\"**Chars:** {len(diff_text)}\",\n            \"\",\n            \"```diff\",\n            diff_text,\n            \"```\",\n        ]))\n\n        # ── Step 2: Parse diff structure ────────────────────────────────\n        changes = parse_unified_diff(diff_text)\n        files_changed = list({c[\"file_path\"] for c in changes})\n        logger.info(f\"Parsed {len(changes)} hunks across {len(files_changed)} files\")\n\n        _write_step(review_dir, \"02_parsed_diff.md\", \"\\n\".join([\n            f\"# Step 2 — Parsed Diff\",\n            f\"**Hunks:** {len(changes)}  |  **Files changed:** {len(files_changed)}\",\n            \"\",\n            \"## Files Changed\",\n            *[f\"- `{f}`\" for f in files_changed],\n            \"\",\n            \"## Hunks\",\n            *[\n                f\"- `{c['file_path']}` lines {c.get('start_line', '?')}–{c.get('end_line', '?')}\"\n                for c in changes\n            ],\n        ]))\n\n        # ── Step 3: Extract imports & symbols from added lines ──────────\n        diff_imports = extract_imports_from_diff(diff_text)\n        diff_functions, diff_classes = extract_symbols_from_diff(diff_text)\n        all_diff_symbols = diff_functions | diff_classes\n        logger.info(\n            f\"Diff extraction: {len(diff_imports)} imports, \"\n            f\"{len(diff_functions)} functions, {len(diff_classes)} classes\"\n        )\n\n        _write_step(review_dir, \"03_extracted_symbols.md\", \"\\n\".join([\n            f\"# Step 3 — Extracted Imports & Symbols\",\n            \"\",\n            f\"## Imports ({len(diff_imports)})\",\n            *[\n                f\"- `{imp.get('name', '?')}` \"\n                f\"(full: `{imp.get('full_import_name', imp.get('source', '?'))}`)\"\n                for imp in diff_imports\n            ],\n            \"\",\n            f\"## Functions ({len(diff_functions)})\",\n            *[f\"- `{fn}`\" for fn in sorted(diff_functions)],\n            \"\",\n            f\"## Classes ({len(diff_classes)})\",\n            *[f\"- `{cls}`\" for cls in sorted(diff_classes)],\n        ]))\n\n        # ── Step 4: Connect to Neo4j and resolve context ────────────────\n        neo4j = Neo4jClient(\n            uri=os.environ.get(\"NEO4J_URI\", \"\"),\n            user=os.environ.get(\"NEO4J_USERNAME\", \"neo4j\"),\n            password=os.environ.get(\"NEO4J_PASSWORD\", \"\"),\n            database=os.environ.get(\"NEO4J_DATABASE\", \"neo4j\"),\n        )\n        query_service = CodeQueryService(neo4j)\n        from api.services.code_search import CodeFinder\n        code_finder = CodeFinder(neo4j)\n\n        # 4a. Graph symbols overlapping the changed line ranges\n        graph_context = query_service.get_diff_context_enhanced(repo_id, changes)\n        graph_symbols = graph_context.get(\"affected_symbols\", [])\n        graph_section = build_graph_context_section(graph_context)\n\n        _write_step(review_dir, \"04a_graph_symbols.md\", \"\\n\".join([\n            f\"# Step 4a — Graph Symbols (overlapping changed lines)\",\n            \"\",\n            f\"## Affected Symbols ({len(graph_symbols)})\",\n            *[\n                f\"- `{s.get('name', '')}` ({s.get('type', 'unknown')}) \"\n                f\"in `{s.get('file_path', '')}`\"\n                for s in graph_symbols\n            ],\n            \"\",\n            \"## Graph Context Section (for agents)\",\n            graph_section,\n        ]))\n\n        # 4b. Look up each imported name in the graph (scoped to this repo)\n        resolved_imports = _resolve_imports_from_graph(neo4j, diff_imports, repo_id)\n        logger.info(f\"Resolved {len(resolved_imports)}/{len(diff_imports)} imports from graph\")\n\n        _write_step(review_dir, \"04b_resolved_imports.md\", \"\\n\".join([\n            f\"# Step 4b — Resolved Imports from Graph\",\n            f\"**Resolved:** {len(resolved_imports)} / {len(diff_imports)}\",\n            \"\",\n            *[\n                \"\\n\".join([\n                    f\"## `{imp['name']}` from `{imp['module']}`\",\n                    f\"- **Path:** `{imp.get('path', 'n/a')}`\",\n                    f\"- **Type:** {imp.get('type', 'n/a')}\",\n                    f\"- **Line:** {imp.get('line', 'n/a')}\",\n                    f\"- **Docstring:** {imp.get('docstring', '') or '_none_'}\",\n                    \"\",\n                    \"```python\",\n                    imp.get(\"source\", \"\") or \"# source not available\",\n                    \"```\",\n                ])\n                for imp in resolved_imports\n            ],\n        ]))\n\n        # 4c. Find callers of changed functions (impact analysis)\n        all_symbol_names = all_diff_symbols | {s.get(\"name\", \"\") for s in graph_symbols}\n        caller_context = _find_callers(code_finder, all_symbol_names)\n        total_callers = sum(len(c[\"callers\"]) for c in caller_context)\n        logger.info(f\"Found {total_callers} callers across {len(caller_context)} symbols\")\n\n        _write_step(review_dir, \"04c_caller_context.md\", \"\\n\".join([\n            f\"# Step 4c — Caller Impact Analysis\",\n            f\"**Total callers:** {total_callers}  |  **Symbols with callers:** {len(caller_context)}\",\n            \"\",\n            *[\n                \"\\n\".join([\n                    f\"## `{entry['symbol']}`\",\n                    *[\n                        f\"- `{c['name']}` in `{c.get('path', '?')}` (line {c.get('line', '?')})\"\n                        for c in entry[\"callers\"]\n                    ],\n                ])\n                for entry in caller_context\n            ],\n        ]))\n\n        # ── Step 5: Determine risk level ────────────────────────────────\n        changed_symbol_names = list(all_symbol_names)\n        if total_callers > 10 or len(all_symbol_names) > 5:\n            risk_level = \"high\"\n        elif total_callers > 3 or len(all_symbol_names) > 2:\n            risk_level = \"medium\"\n        else:\n            risk_level = \"low\"\n\n        _write_step(review_dir, \"05_risk_level.md\", \"\\n\".join([\n            f\"# Step 5 — Risk Level\",\n            \"\",\n            f\"**Risk level:** `{risk_level}`\",\n            f\"**Total callers:** {total_callers}\",\n            f\"**Total symbols:** {len(all_symbol_names)}\",\n            \"\",\n            \"## All Changed Symbols\",\n            *[f\"- `{s}`\" for s in sorted(changed_symbol_names)],\n        ]))\n\n        # ── Step 6: Build context for LLM agents ───────────────────────\n        agent_context = _build_agent_context(\n            resolved_imports, caller_context, graph_section\n        )\n\n        _write_step(review_dir, \"06_agent_context.md\", \"\\n\".join([\n            f\"# Step 6 — Agent Context (what would be sent to the LLM)\",\n            \"\",\n            agent_context,\n        ]))\n\n        # ── Step 7: LLM call skipped (debug mode) ──────────────────────\n        logger.info(\n            f\"[DEBUG MODE] LLM call skipped. \"\n            f\"All context written to {review_dir}\"\n        )\n\n    except Exception:\n        logger.error(f\"Review pipeline failed: {traceback.format_exc()}\")","originalFile":"\"\"\"\nPR Review Service\n=================\nOrchestrates the full review pipeline:\n  1. Fetch PR diff from GitHub\n  2. Parse diff → extract added source per file\n  3. Use ingestion_service language parsers (tree-sitter) to extract imports & symbols\n  4. Resolve those imports against the Neo4j graph (same repo)\n  5. Find callers of changed functions (impact analysis)\n  6. Build context → run LLM agents → post comment\n\"\"\"\n\nimport importlib\nimport logging\nimport os\nimport traceback\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, List, Set, Tuple\n\nfrom api.utils.comment_formatter import format_github_comment\nfrom api.utils.graph_context import build_graph_context_section\nfrom db.client import Neo4jClient\nfrom db.queries import CodeQueryService\nfrom deepagent.models.agent_schemas import ContextData\nfrom deepagent.agent.review_pipeline import run_review\nfrom common.github_client import GitHubClient\nfrom common.diff_parser import parse_unified_diff\n\nlogger = logging.getLogger(__name__)\n\n\n# ==========================================================================\n# File extension → tree-sitter language name mapping (all 17 supported)\n# ==========================================================================\n_EXT_TO_LANG = {\n    \".py\": \"python\",\n    \".js\": \"javascript\", \".jsx\": \"javascript\", \".mjs\": \"javascript\", \".cjs\": \"javascript\",\n    \".ts\": \"typescript\", \".tsx\": \"typescript\",\n    \".go\": \"go\",\n    \".rs\": \"rust\",\n    \".java\": \"java\",\n    \".rb\": \"ruby\",\n    \".c\": \"c\",\n    \".cpp\": \"cpp\", \".h\": \"cpp\", \".hpp\": \"cpp\",\n    \".cs\": \"c_sharp\",\n    \".kt\": \"kotlin\",\n    \".scala\": \"scala\", \".sc\": \"scala\",\n    \".swift\": \"swift\",\n    \".php\": \"php\",\n    \".hs\": \"haskell\",\n}\n\n# ==========================================================================\n# Language parser registry\n# Maps language name → (module path, class name) for lazy import.\n# These are the SAME parsers used by the ingestion service to build the graph.\n# ==========================================================================\n_LANG_PARSER_REGISTRY = {\n    \"python\":     (\"ingestion_service.languages.python\",     \"PythonLangTreeSitterParser\"),\n    \"javascript\": (\"ingestion_service.languages.javascript\", \"JavascriptLangTreeSitterParser\"),\n    \"typescript\": (\"ingestion_service.languages.typescript\", \"TypescriptLangTreeSitterParser\"),\n    \"go\":         (\"ingestion_service.languages.go\",         \"GoLangTreeSitterParser\"),\n    \"java\":       (\"ingestion_service.languages.java\",       \"JavaLangTreeSitterParser\"),\n    \"rust\":       (\"ingestion_service.languages.rust\",       \"RustLangTreeSitterParser\"),\n    \"c\":          (\"ingestion_service.languages.c\",          \"CLangTreeSitterParser\"),\n    \"cpp\":        (\"ingestion_service.languages.cpp\",        \"CppLangTreeSitterParser\"),\n    \"ruby\":       (\"ingestion_service.languages.ruby\",       \"RubyLangTreeSitterParser\"),\n    \"c_sharp\":    (\"ingestion_service.languages.csharp\",     \"CSharpLangTreeSitterParser\"),\n    \"php\":        (\"ingestion_service.languages.php\",        \"PhpLangTreeSitterParser\"),\n    \"kotlin\":     (\"ingestion_service.languages.kotlin\",     \"KotlinLangTreeSitterParser\"),\n    \"scala\":      (\"ingestion_service.languages.scala\",      \"ScalaLangTreeSitterParser\"),\n    \"swift\":      (\"ingestion_service.languages.swift\",      \"SwiftLangTreeSitterParser\"),\n    \"haskell\":    (\"ingestion_service.languages.haskell\",    \"HaskellLangTreeSitterParser\"),\n}\n\n# Cache so we only instantiate each language parser once\n_parser_cache: Dict[str, object] = {}\n\n\n# ==========================================================================\n# Parser loading\n# ==========================================================================\n\ndef _get_lang_parser(lang: str):\n    \"\"\"\n    Get the tree-sitter language parser for `lang`.\n\n    Each language parser (e.g. PythonLangTreeSitterParser) expects a wrapper\n    with .language_name, .language, and .parser attributes.  We build a\n    lightweight adapter instead of pulling in the full ingestion TreeSitterParser\n    which depends on Neo4j/jobs.\n    \"\"\"\n    # Return from cache if already loaded\n    if lang in _parser_cache:\n        return _parser_cache[lang]\n\n    # Unknown language → skip\n    if lang not in _LANG_PARSER_REGISTRY:\n        return None\n\n    module_path, class_name = _LANG_PARSER_REGISTRY[lang]\n    try:\n        from common.tree_sitter_manager import get_language_safe, create_parser\n\n        # Build the adapter the language parser constructors expect\n        class _Adapter:\n            pass\n        adapter = _Adapter()\n        adapter.language_name = lang\n        adapter.language = get_language_safe(lang)   # tree-sitter Language object\n        adapter.parser = create_parser(lang)         # tree-sitter Parser object\n\n        # Dynamically import the language-specific parser class\n        module = importlib.import_module(module_path)\n        parser_class = getattr(module, class_name)\n\n        # Instantiate and configure\n        parser_instance = parser_class(adapter)\n        parser_instance.index_source = False  # we don't need source code stored on each node\n\n        _parser_cache[lang] = parser_instance\n        return parser_instance\n    except Exception as e:\n        logger.warning(f\"Failed to load parser for {lang}: {e}\")\n        return None\n\n\n# ==========================================================================\n# Diff parsing helpers\n# ==========================================================================\n\ndef _extract_added_source_by_file(diff_text: str) -> Dict[str, str]:\n    \"\"\"\n    Walk through a unified diff and collect added lines per file.\n\n    Returns {\"app/services/foo.py\": \"from x import y\\\\ndef bar(): ...\"}\n\n    Only lines starting with '+' (but not '+++ b/...') are collected,\n    with the leading '+' stripped so tree-sitter can parse the source.\n    \"\"\"\n    files: Dict[str, List[str]] = {}\n    current_file = None\n\n    for line in diff_text.splitlines():\n        # \"+++ b/path/to/file\" marks the start of a new file in the diff\n        if line.startswith(\"+++ b/\"):\n            current_file = line[6:]  # strip \"+++ b/\" prefix\n            if current_file not in files:\n                files[current_file] = []\n        # Lines starting with \"+\" are added lines (skip the \"+++ b/\" header)\n        elif line.startswith(\"+\") and not line.startswith(\"+++\") and current_file:\n            files[current_file].append(line[1:])  # strip the leading \"+\"\n\n    # Join lines into a single source string per file, skip empty files\n    return {f: \"\\n\".join(lines) for f, lines in files.items() if lines}\n\n\n# ==========================================================================\n# Import & symbol extraction from diff (tree-sitter powered)\n# ==========================================================================\n\ndef extract_imports_from_diff(diff_text: str) -> List[Dict]:\n    \"\"\"\n    Extract imports from the added lines of a diff using tree-sitter.\n\n    Uses the same language parsers as the ingestion service, so the import\n    format matches what the graph stores:\n      Python:     {\"name\": \"foo\", \"full_import_name\": \"bar.foo\", ...}\n      JavaScript: {\"name\": \"useState\", \"source\": \"react\", ...}\n    \"\"\"\n    all_imports: List[Dict] = []\n\n    # Group added source by file path\n    added_source = _extract_added_source_by_file(diff_text)\n\n    for file_path, source in added_source.items():\n        # Determine language from file extension\n        ext = Path(file_path).suffix.lower()\n        lang = _EXT_TO_LANG.get(ext)\n        if not lang:\n            continue\n\n        # Load the language-specific parser\n        parser = _get_lang_parser(lang)\n        if not parser or not hasattr(parser, \"_find_imports\"):\n            continue\n\n        try:\n            # Parse the added source into a tree-sitter AST\n            tree = parser.parser.parse(source.encode(\"utf-8\"))\n            # Use the parser's own _find_imports to extract structured import data\n            file_imports = parser._find_imports(tree.root_node)\n            all_imports.extend(file_imports)\n        except Exception as e:\n            logger.warning(f\"Import extraction failed for {file_path}: {e}\")\n\n    return all_imports\n\n\ndef extract_symbols_from_diff(diff_text: str) -> Tuple[Set[str], Set[str]]:\n    \"\"\"\n    Extract function and class names defined in the added lines of a diff.\n\n    Returns (function_names, class_names) as sets of strings.\n    \"\"\"\n    functions: Set[str] = set()\n    classes: Set[str] = set()\n\n    added_source = _extract_added_source_by_file(diff_text)\n\n    print('----- The Added source for the diff -----')\n    print(added_source)\n    print('-----------------------------------------------------')\n\n    for file_path, source in added_source.items():\n        #get the file extension\n        ext = Path(file_path).suffix.lower()\n\n        #get the language from the file extension\n        lang = _EXT_TO_LANG.get(ext)\n        if not lang:\n            continue\n\n        #get the language-specific parser\n        parser = _get_lang_parser(lang)\n        if not parser:\n            continue\n\n        try:\n            #parse the added source into a tree-sitter AST\n            tree = parser.parser.parse(source.encode(\"utf-8\"))\n            root = tree.root_node\n\n            # Extract function definitions\n            if hasattr(parser, \"_find_functions\"):\n                for func in parser._find_functions(root):\n                    functions.add(func.get(\"name\", \"\"))\n\n            # Extract class definitions\n            if hasattr(parser, \"_find_classes\"):\n                for cls in parser._find_classes(root):\n                    classes.add(cls.get(\"name\", \"\"))\n        except Exception as e:\n            logger.warning(f\"Symbol extraction failed for {file_path}: {e}\")\n\n    return functions, classes\n\n\n# ==========================================================================\n# Graph-based resolution\n# ==========================================================================\n\ndef _resolve_imports_from_graph(\n    neo4j_client: Neo4jClient, imports: List[Dict], repo_id: str,\n) -> List[Dict]:\n    \"\"\"\n    For each imported name, search the Neo4j graph for its definition.\n\n    Queries directly with repo filtering (WHERE n.repo = $repo) so results\n    are scoped to the PR's repository. Searches Function, Class, and Variable\n    nodes by exact name match.\n\n    Returns a list of resolved import dicts with source code from the graph.\n    \"\"\"\n    resolved = []\n    seen = set()  # avoid looking up the same name twice\n\n    with neo4j_client.driver.session() as session:\n        for imp in imports:\n            name = imp.get(\"name\", \"\")\n\n            # Skip empty, already-seen, or wildcard imports\n            if not name or name in seen or name == \"*\":\n                continue\n            seen.add(name)\n\n            # full_import_name for Python (\"app.services.foo.bar\"),\n            # source for JS/TS (\"react\", \"./utils\")\n            module = imp.get(\"full_import_name\", imp.get(\"source\", \"\"))\n\n            try:\n                # Search for the symbol in the graph, scoped to this repo\n                result = session.run(\"\"\"\n                    MATCH (n)\n                    WHERE (n:Function OR n:Class OR n:Variable)\n                      AND n.name = $name\n                      AND n.repo = $repo\n                    RETURN n.name AS name, n.path AS path,\n                           n.line_number AS line_number,\n                           n.source AS source, n.docstring AS docstring,\n                           labels(n) AS labels\n                    LIMIT 1\n                \"\"\", name=name, repo=repo_id)\n\n                records = result.data()\n                if not records:\n                    continue  # symbol not in graph (external dep or not ingested)\n\n                r = records[0]\n                labels = r.get(\"labels\", [])\n                node_type = \"class\" if \"Class\" in labels else \"function\"\n\n                resolved.append({\n                    \"name\": name,\n                    \"module\": module,\n                    \"type\": node_type,\n                    \"source\": r.get(\"source\", \"\"),\n                    \"path\": r.get(\"path\", \"\"),\n                    \"line\": r.get(\"line_number\"),\n                    \"docstring\": r.get(\"docstring\", \"\"),\n                })\n            except Exception as e:\n                logger.warning(f\"Failed to resolve import {name}: {e}\")\n\n    return resolved\n\n\ndef _find_callers(code_finder, symbol_names: Set[str]) -> List[Dict]:\n    \"\"\"\n    For each changed symbol, find functions that call it (impact analysis).\n\n    Uses CodeFinder.who_calls_function which queries CALLS relationships\n    in the graph.  Returns at most 10 callers per symbol.\n    \"\"\"\n    callers = []\n\n    for name in symbol_names:\n        try:\n            results = code_finder.who_calls_function(name)\n            if not results:\n                continue\n\n            # Extract caller info, limit to 10 per symbol\n            caller_list = [\n                {\n                    \"name\": c.get(\"caller_function\"),\n                    \"path\": c.get(\"caller_file_path\"),\n                    \"line\": c.get(\"caller_line_number\"),\n                }\n                for c in results\n                if c.get(\"caller_function\")\n            ][:10]\n\n            if caller_list:\n                callers.append({\"symbol\": name, \"callers\": caller_list})\n        except Exception as e:\n            logger.warning(f\"Failed to find callers for {name}: {e}\")\n\n    return callers\n\n\n# ==========================================================================\n# Context formatting (for LLM agents)\n# ==========================================================================\n\ndef _build_agent_context(\n    import_context: List[Dict],\n    caller_context: List[Dict],\n    graph_section: str,\n) -> str:\n    \"\"\"\n    Build the markdown context string that gets sent to the LLM review agents.\n\n    Combines:\n      - Resolved import source code from the graph\n      - Impact analysis (who calls the changed code)\n      - Graph context (symbols in changed line ranges)\n    \"\"\"\n    parts = []\n\n    # Section 1: Source code of imported dependencies\n    if import_context:\n        parts.append(\"## Imported Dependencies (source from graph)\")\n        for imp in import_context:\n            source = imp.get(\"source\", \"\")\n            if source:\n                parts.append(f\"### `{imp['name']}` from `{imp['module']}` ({imp.get('path', '')})\")\n                parts.append(f\"```python\\n{source}\\n```\")\n            else:\n                parts.append(f\"- `{imp['name']}` from `{imp['module']}` (source not in graph)\")\n        parts.append(\"\")\n\n    # Section 2: Callers of changed functions\n    if caller_context:\n        parts.append(\"## Impact Analysis (who calls the changed code)\")\n        for entry in caller_context:\n            caller_names = \", \".join(\n                f\"`{c['name']}` in `{c.get('path', '?')}`\" for c in entry[\"callers\"]\n            )\n            parts.append(f\"- `{entry['symbol']}` is called by: {caller_names}\")\n        parts.append(\"\")\n\n    # Section 3: Graph context (symbols overlapping changed lines)\n    if graph_section and graph_section != \"No graph context available.\":\n        parts.append(\"## Graph Context\")\n        parts.append(graph_section)\n        parts.append(\"\")\n\n    return \"\\n\".join(parts) if parts else \"No additional context available.\"\n\n\n# ==========================================================================\n# Per-step debug dump helpers\n# ==========================================================================\n\ndef _make_review_dir(owner: str, repo: str, pr_number: int) -> Path:\n    \"\"\"Create and return a timestamped folder for this review run.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n    review_dir = Path(\"output\") / f\"review-{timestamp}\"\n    review_dir.mkdir(parents=True, exist_ok=True)\n    logger.info(f\"Review debug dir: {review_dir}\")\n    return review_dir\n\n\ndef _write_step(review_dir: Path, filename: str, content: str) -> None:\n    \"\"\"Write a single step's debug output to <review_dir>/<filename>.md.\"\"\"\n    try:\n        (review_dir / filename).write_text(content, encoding=\"utf-8\")\n    except Exception as e:\n        logger.warning(f\"Failed to write {filename}: {e}\")\n\n\n# ==========================================================================\n# Main pipeline\n# ==========================================================================\n\nasync def execute_pr_review(owner: str, repo: str, pr_number: int) -> None:\n    \"\"\"\n    Full PR review pipeline.\n\n    Steps:\n      1. Fetch the PR diff from GitHub\n      2. Parse the diff into changed file paths and line ranges\n      3. Extract imports and symbol definitions from added lines (tree-sitter)\n      4. Connect to Neo4j and resolve context:\n         a. Find graph symbols overlapping changed line ranges\n         b. Look up imported symbols in the graph (get source code)\n         c. Find callers of changed functions (impact analysis)\n      5. Build risk level based on scope of changes\n      6. Format everything into context for the LLM agents\n      7. Dump context to file for debugging\n      8. Run multi-agent review (bug-hunter + security-auditor)\n      9. Format and post the review comment on the PR\n    \"\"\"\n    try:\n        logger.info(f\"Starting review pipeline for {owner}/{repo}#{pr_number}\")\n        repo_id = f\"{owner}/{repo}\"  # matches the repo identifier stored in the graph\n\n        # ── Step 1: Fetch diff from GitHub ──────────────────────────────\n        gh = GitHubClient()\n        diff_text = await gh.get_pr_diff(owner, repo, pr_number)\n        if not diff_text:\n            logger.warning(\"Empty diff, skipping review\")\n            return\n        logger.info(f\"Fetched diff ({len(diff_text)} chars)\")\n\n        # ── Step 2: Parse diff structure ────────────────────────────────\n        # Returns list of {\"file_path\", \"start_line\", \"end_line\"} hunks\n        changes = parse_unified_diff(diff_text)\n        files_changed = list({c[\"file_path\"] for c in changes})\n        logger.info(f\"Parsed {len(changes)} hunks across {len(files_changed)} files\")\n\n        # ── Step 3: Extract imports & symbols from added lines ──────────\n        # Uses the same tree-sitter language parsers as the ingestion service\n        diff_imports = extract_imports_from_diff(diff_text)\n\n        print('-----------------------------------------------------')\n        print(diff_imports)\n        print('-----------------------------------------------------')\n\n        diff_functions, diff_classes = extract_symbols_from_diff(diff_text)\n\n        print('-----------------------------------------------------')\n        print(diff_functions)\n        print(diff_classes)\n        print('-----------------------------------------------------')\n\n    \n        all_diff_symbols = diff_functions | diff_classes\n        logger.info(\n            f\"Diff extraction: {len(diff_imports)} imports, \"\n            f\"{len(diff_functions)} functions, {len(diff_classes)} classes\"\n        )\n\n        # ── Step 4: Connect to Neo4j and resolve context ────────────────\n        neo4j = Neo4jClient(\n            uri=os.environ.get(\"NEO4J_URI\", \"\"),\n            user=os.environ.get(\"NEO4J_USERNAME\", \"neo4j\"),\n            password=os.environ.get(\"NEO4J_PASSWORD\", \"\"),\n            database=os.environ.get(\"NEO4J_DATABASE\", \"neo4j\"),\n        )\n        query_service = CodeQueryService(neo4j)\n        from api.services.code_search import CodeFinder\n        code_finder = CodeFinder(neo4j)\n\n        # 4a. Graph symbols overlapping the changed line ranges\n        graph_context = query_service.get_diff_context_enhanced(repo_id, changes)\n        graph_symbols = graph_context.get(\"affected_symbols\", [])\n        graph_section = build_graph_context_section(graph_context)\n\n        # 4b. Look up each imported name in the graph (scoped to this repo)\n        resolved_imports = _resolve_imports_from_graph(neo4j, diff_imports, repo_id)\n        logger.info(f\"Resolved {len(resolved_imports)}/{len(diff_imports)} imports from graph\")\n\n        # 4c. Find callers of changed functions (impact analysis)\n        # Merge symbols from diff extraction + graph overlap\n        all_symbol_names = all_diff_symbols | {s.get(\"name\", \"\") for s in graph_symbols}\n        caller_context = _find_callers(code_finder, all_symbol_names)\n        total_callers = sum(len(c[\"callers\"]) for c in caller_context)\n        logger.info(f\"Found {total_callers} callers across {len(caller_context)} symbols\")\n\n        # ── Step 5: Determine risk level ────────────────────────────────\n        changed_symbol_names = list(all_symbol_names)\n        if total_callers > 10 or len(all_symbol_names) > 5:\n            risk_level = \"high\"\n        elif total_callers > 3 or len(all_symbol_names) > 2:\n            risk_level = \"medium\"\n        else:\n            risk_level = \"low\"\n\n        # ── Step 6: Build context for LLM agents ───────────────────────\n        agent_context = _build_agent_context(\n            resolved_imports, caller_context, graph_section\n        )\n\n        # ── Step 7: Dump context to file for debugging ──────────────────\n        _dump_context_to_file(\n            owner, repo, pr_number, diff_text,\n            agent_context, diff_imports, resolved_imports, caller_context,\n            graph_symbols, files_changed, changed_symbol_names, risk_level,\n        )\n\n        # ── Step 8: Run multi-agent review ──────────────────────────────\n        review_results = await run_review(diff_text, agent_context, repo_id, pr_number)\n        logger.info(f\"Review complete: {len(review_results.issues)} issues found\")\n\n        # ── Step 9: Format and post comment ─────────────────────────────\n        context_data = ContextData(\n            files_changed=files_changed,\n            modified_symbols=changed_symbol_names,\n            total_callers=total_callers,\n            risk_level=risk_level,\n        )\n        final_comment = format_github_comment(review_results, context_data, pr_number)\n        await gh.post_comment(owner, repo, pr_number, final_comment)\n        logger.info(f\"Posted review comment on {owner}/{repo}#{pr_number}\")\n\n    except Exception:\n        logger.error(f\"Review pipeline failed: {traceback.format_exc()}\")\n","structuredPatch":[{"oldStart":439,"oldLines":14,"newStart":439,"newLines":15,"lines":["          c. Find callers of changed functions (impact analysis)","       5. Build risk level based on scope of changes","       6. Format everything into context for the LLM agents","-      7. Dump context to file for debugging","-      8. Run multi-agent review (bug-hunter + security-auditor)","-      9. Format and post the review comment on the PR","+      7. [DEBUG MODE] LLM call skipped — all context written to review dir","     \"\"\"","     try:","         logger.info(f\"Starting review pipeline for {owner}/{repo}#{pr_number}\")","         repo_id = f\"{owner}/{repo}\"  # matches the repo identifier stored in the graph"," ","+        # Create a timestamped folder for this review run","+        review_dir = _make_review_dir(owner, repo, pr_number)","+","         # ── Step 1: Fetch diff from GitHub ──────────────────────────────","         gh = GitHubClient()","         diff_text = await gh.get_pr_diff(owner, repo, pr_number)"]},{"oldStart":455,"oldLines":34,"newStart":456,"newLines":61,"lines":["             return","         logger.info(f\"Fetched diff ({len(diff_text)} chars)\")"," ","+        _write_step(review_dir, \"01_diff.md\", \"\\n\".join([","+            f\"# Step 1 — Raw Diff\",","+            f\"**PR:** {owner}/{repo}#{pr_number}\",","+            f\"**Chars:** {len(diff_text)}\",","+            \"\",","+            \"```diff\",","+            diff_text,","+            \"```\",","+        ]))","+","         # ── Step 2: Parse diff structure ────────────────────────────────","-        # Returns list of {\"file_path\", \"start_line\", \"end_line\"} hunks","         changes = parse_unified_diff(diff_text)","         files_changed = list({c[\"file_path\"] for c in changes})","         logger.info(f\"Parsed {len(changes)} hunks across {len(files_changed)} files\")"," ","+        _write_step(review_dir, \"02_parsed_diff.md\", \"\\n\".join([","+            f\"# Step 2 — Parsed Diff\",","+            f\"**Hunks:** {len(changes)}  |  **Files changed:** {len(files_changed)}\",","+            \"\",","+            \"## Files Changed\",","+            *[f\"- `{f}`\" for f in files_changed],","+            \"\",","+            \"## Hunks\",","+            *[","+                f\"- `{c['file_path']}` lines {c.get('start_line', '?')}–{c.get('end_line', '?')}\"","+                for c in changes","+            ],","+        ]))","+","         # ── Step 3: Extract imports & symbols from added lines ──────────","-        # Uses the same tree-sitter language parsers as the ingestion service","         diff_imports = extract_imports_from_diff(diff_text)","-","-        print('-----------------------------------------------------')","-        print(diff_imports)","-        print('-----------------------------------------------------')","-","         diff_functions, diff_classes = extract_symbols_from_diff(diff_text)","-","-        print('-----------------------------------------------------')","-        print(diff_functions)","-        print(diff_classes)","-        print('-----------------------------------------------------')","-","-    ","         all_diff_symbols = diff_functions | diff_classes","         logger.info(","             f\"Diff extraction: {len(diff_imports)} imports, \"","             f\"{len(diff_functions)} functions, {len(diff_classes)} classes\"","         )"," ","+        _write_step(review_dir, \"03_extracted_symbols.md\", \"\\n\".join([","+            f\"# Step 3 — Extracted Imports & Symbols\",","+            \"\",","+            f\"## Imports ({len(diff_imports)})\",","+            *[","+                f\"- `{imp.get('name', '?')}` \"","+                f\"(full: `{imp.get('full_import_name', imp.get('source', '?'))}`)\"","+                for imp in diff_imports","+            ],","+            \"\",","+            f\"## Functions ({len(diff_functions)})\",","+            *[f\"- `{fn}`\" for fn in sorted(diff_functions)],","+            \"\",","+            f\"## Classes ({len(diff_classes)})\",","+            *[f\"- `{cls}`\" for cls in sorted(diff_classes)],","+        ]))","+","         # ── Step 4: Connect to Neo4j and resolve context ────────────────","         neo4j = Neo4jClient(","             uri=os.environ.get(\"NEO4J_URI\", \"\"),"]},{"oldStart":499,"oldLines":17,"newStart":527,"newLines":66,"lines":["         graph_symbols = graph_context.get(\"affected_symbols\", [])","         graph_section = build_graph_context_section(graph_context)"," ","+        _write_step(review_dir, \"04a_graph_symbols.md\", \"\\n\".join([","+            f\"# Step 4a — Graph Symbols (overlapping changed lines)\",","+            \"\",","+            f\"## Affected Symbols ({len(graph_symbols)})\",","+            *[","+                f\"- `{s.get('name', '')}` ({s.get('type', 'unknown')}) \"","+                f\"in `{s.get('file_path', '')}`\"","+                for s in graph_symbols","+            ],","+            \"\",","+            \"## Graph Context Section (for agents)\",","+            graph_section,","+        ]))","+","         # 4b. Look up each imported name in the graph (scoped to this repo)","         resolved_imports = _resolve_imports_from_graph(neo4j, diff_imports, repo_id)","         logger.info(f\"Resolved {len(resolved_imports)}/{len(diff_imports)} imports from graph\")"," ","+        _write_step(review_dir, \"04b_resolved_imports.md\", \"\\n\".join([","+            f\"# Step 4b — Resolved Imports from Graph\",","+            f\"**Resolved:** {len(resolved_imports)} / {len(diff_imports)}\",","+            \"\",","+            *[","+                \"\\n\".join([","+                    f\"## `{imp['name']}` from `{imp['module']}`\",","+                    f\"- **Path:** `{imp.get('path', 'n/a')}`\",","+                    f\"- **Type:** {imp.get('type', 'n/a')}\",","+                    f\"- **Line:** {imp.get('line', 'n/a')}\",","+                    f\"- **Docstring:** {imp.get('docstring', '') or '_none_'}\",","+                    \"\",","+                    \"```python\",","+                    imp.get(\"source\", \"\") or \"# source not available\",","+                    \"```\",","+                ])","+                for imp in resolved_imports","+            ],","+        ]))","+","         # 4c. Find callers of changed functions (impact analysis)","-        # Merge symbols from diff extraction + graph overlap","         all_symbol_names = all_diff_symbols | {s.get(\"name\", \"\") for s in graph_symbols}","         caller_context = _find_callers(code_finder, all_symbol_names)","         total_callers = sum(len(c[\"callers\"]) for c in caller_context)","         logger.info(f\"Found {total_callers} callers across {len(caller_context)} symbols\")"," ","+        _write_step(review_dir, \"04c_caller_context.md\", \"\\n\".join([","+            f\"# Step 4c — Caller Impact Analysis\",","+            f\"**Total callers:** {total_callers}  |  **Symbols with callers:** {len(caller_context)}\",","+            \"\",","+            *[","+                \"\\n\".join([","+                    f\"## `{entry['symbol']}`\",","+                    *[","+                        f\"- `{c['name']}` in `{c.get('path', '?')}` (line {c.get('line', '?')})\"","+                        for c in entry[\"callers\"]","+                    ],","+                ])","+                for entry in caller_context","+            ],","+        ]))","+","         # ── Step 5: Determine risk level ────────────────────────────────","         changed_symbol_names = list(all_symbol_names)","         if total_callers > 10 or len(all_symbol_names) > 5:"]},{"oldStart":519,"oldLines":32,"newStart":596,"newLines":33,"lines":["         else:","             risk_level = \"low\""," ","+        _write_step(review_dir, \"05_risk_level.md\", \"\\n\".join([","+            f\"# Step 5 — Risk Level\",","+            \"\",","+            f\"**Risk level:** `{risk_level}`\",","+            f\"**Total callers:** {total_callers}\",","+            f\"**Total symbols:** {len(all_symbol_names)}\",","+            \"\",","+            \"## All Changed Symbols\",","+            *[f\"- `{s}`\" for s in sorted(changed_symbol_names)],","+        ]))","+","         # ── Step 6: Build context for LLM agents ───────────────────────","         agent_context = _build_agent_context(","             resolved_imports, caller_context, graph_section","         )"," ","-        # ── Step 7: Dump context to file for debugging ──────────────────","-        _dump_context_to_file(","-            owner, repo, pr_number, diff_text,","-            agent_context, diff_imports, resolved_imports, caller_context,","-            graph_symbols, files_changed, changed_symbol_names, risk_level,","-        )","+        _write_step(review_dir, \"06_agent_context.md\", \"\\n\".join([","+            f\"# Step 6 — Agent Context (what would be sent to the LLM)\",","+            \"\",","+            agent_context,","+        ]))"," ","-        # ── Step 8: Run multi-agent review ──────────────────────────────","-        review_results = await run_review(diff_text, agent_context, repo_id, pr_number)","-        logger.info(f\"Review complete: {len(review_results.issues)} issues found\")","-","-        # ── Step 9: Format and post comment ─────────────────────────────","-        context_data = ContextData(","-            files_changed=files_changed,","-            modified_symbols=changed_symbol_names,","-            total_callers=total_callers,","-            risk_level=risk_level,","+        # ── Step 7: LLM call skipped (debug mode) ──────────────────────","+        logger.info(","+            f\"[DEBUG MODE] LLM call skipped. \"","+            f\"All context written to {review_dir}\"","         )","-        final_comment = format_github_comment(review_results, context_data, pr_number)","-        await gh.post_comment(owner, repo, pr_number, final_comment)","-        logger.info(f\"Posted review comment on {owner}/{repo}#{pr_number}\")"," ","     except Exception:","         logger.error(f\"Review pipeline failed: {traceback.format_exc()}\")"]}],"userModified":false,"replaceAll":false},"sourceToolAssistantUUID":"54716037-b591-4a4b-aa14-21777d955898"}
{"parentUuid":"08b4b784-c2ef-49eb-be86-498f0d974273","isSidechain":false,"userType":"external","cwd":"/Users/skmabudalam/Documents/BugViper","sessionId":"0f25fa6f-5425-4cfc-8c2e-2b3c7eda02c7","version":"2.1.39","gitBranch":"review-pipeline-improvements","slug":"immutable-launching-music","type":"assistant","uuid":"f6c6f9b8-46ad-4d6e-8876-e0c0f5372cc2","timestamp":"2026-02-13T07:32:56.107Z","message":{"id":"6accd009-ffa4-4d03-b10d-baac6cfc7fa0","container":null,"model":"<synthetic>","role":"assistant","stop_reason":"stop_sequence","stop_sequence":"","type":"message","usage":{"input_tokens":0,"output_tokens":0,"cache_creation_input_tokens":0,"cache_read_input_tokens":0,"server_tool_use":{"web_search_requests":0,"web_fetch_requests":0},"service_tier":null,"cache_creation":{"ephemeral_1h_input_tokens":0,"ephemeral_5m_input_tokens":0},"inference_geo":null,"iterations":null},"content":[{"type":"text","text":"You've hit your limit · resets 5:30pm (Asia/Calcutta)"}],"context_management":null},"error":"rate_limit","isApiErrorMessage":true}
{"parentUuid":"f6c6f9b8-46ad-4d6e-8876-e0c0f5372cc2","isSidechain":false,"userType":"external","cwd":"/Users/skmabudalam/Documents/BugViper","sessionId":"0f25fa6f-5425-4cfc-8c2e-2b3c7eda02c7","version":"2.1.39","gitBranch":"review-pipeline-improvements","slug":"immutable-launching-music","type":"progress","data":{"type":"hook_progress","hookEvent":"Stop","hookName":"Stop","command":"entire hooks claude-code stop"},"parentToolUseID":"ab56993a-bbbe-42e1-84e6-af0384d899db","toolUseID":"ab56993a-bbbe-42e1-84e6-af0384d899db","timestamp":"2026-02-13T07:32:56.110Z","uuid":"a4934ff3-3afc-4658-85ba-67af8643daf3"}
{"type":"queue-operation","operation":"enqueue","timestamp":"2026-02-13T07:32:56.135Z","sessionId":"0f25fa6f-5425-4cfc-8c2e-2b3c7eda02c7","content":"/rate-limit-options"}
{"type":"queue-operation","operation":"dequeue","timestamp":"2026-02-13T07:32:57.720Z","sessionId":"0f25fa6f-5425-4cfc-8c2e-2b3c7eda02c7"}
{"parentUuid":"a4934ff3-3afc-4658-85ba-67af8643daf3","isSidechain":false,"userType":"external","cwd":"/Users/skmabudalam/Documents/BugViper","sessionId":"0f25fa6f-5425-4cfc-8c2e-2b3c7eda02c7","version":"2.1.39","gitBranch":"review-pipeline-improvements","slug":"immutable-launching-music","type":"system","subtype":"stop_hook_summary","hookCount":1,"hookInfos":[{"command":"entire hooks claude-code stop"}],"hookErrors":[],"preventedContinuation":false,"stopReason":"","hasOutput":true,"level":"suggestion","timestamp":"2026-02-13T07:32:57.639Z","uuid":"6bfe9176-9997-4ce2-b1dd-44f4fee32209","toolUseID":"ab56993a-bbbe-42e1-84e6-af0384d899db"}
{"parentUuid":"6bfe9176-9997-4ce2-b1dd-44f4fee32209","isSidechain":false,"userType":"external","cwd":"/Users/skmabudalam/Documents/BugViper","sessionId":"0f25fa6f-5425-4cfc-8c2e-2b3c7eda02c7","version":"2.1.39","gitBranch":"review-pipeline-improvements","slug":"immutable-launching-music","type":"system","subtype":"turn_duration","durationMs":81186,"timestamp":"2026-02-13T07:32:57.641Z","uuid":"0c2fc763-6b7d-41d3-88bf-f55b2be5c4da","isMeta":false}
